<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="https://1kunn.github.io/atom.xml" rel="self"/>
  
  <link href="https://1kunn.github.io/"/>
  <updated>2024-09-13T07:34:24.693Z</updated>
  <id>https://1kunn.github.io/</id>
  
  <author>
    <name>HRW</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LeNet和AlexNet实现</title>
    <link href="https://1kunn.github.io/2024/09/12/LeNet%E5%92%8CAlexNet%E5%AE%9E%E7%8E%B0/"/>
    <id>https://1kunn.github.io/2024/09/12/LeNet%E5%92%8CAlexNet%E5%AE%9E%E7%8E%B0/</id>
    <published>2024-09-12T01:36:28.000Z</published>
    <updated>2024-09-13T07:34:24.693Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CNN经典模型LeNet实现笔记"><a href="#CNN经典模型LeNet实现笔记" class="headerlink" title="CNN经典模型LeNet实现笔记"></a>CNN经典模型LeNet实现笔记</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>视频教程：<a href="https://www.bilibili.com/video/BV1e34y1M7wR/?p=42&spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=49378ece86ab83ee76bcaac1cc96562c">https://www.bilibili.com/video/BV1e34y1M7wR/?p=42&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=49378ece86ab83ee76bcaac1cc96562c</a></p><p>环境：python3.9</p><p>​PyTorch&#x3D;conda install pytorch&#x3D;&#x3D;2.0.0 torchvision&#x3D;&#x3D;0.15.0 torchaudio&#x3D;&#x3D;2.0.0 pytorch-cuda&#x3D;11.8 -c pytorch -c nvidia</p><h1 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h1><p>第1层输入层: Input为28x28x1</p><p>第2层卷积层: Input为28x28x1 ,卷积核5x5 x1x 6; stride &#x3D; 1,padding&#x3D;2。output为28x28x6</p><p>第3层平均池化层: Input为28x28x6， 池化感受野为2x2，stride &#x3D; 2. output为14x14x6</p><p>第4层卷积层: Input为14x14x6， 卷积核5x5x6 x16， stride &#x3D; 1, padding&#x3D;0, output为10x10x16</p><p>第5层平均池化层: Input为10x10x16, 池化感受野为2x2，stride &#x3D; 2, output为5x5x16，Flatten操作， 通过展平得到400个数据与之后的全连接层相连。</p><p>第6~ 8层全连接层:<br>第6~8层神经元个数分别为120，84, 10。其中神经网络中用sigmoid作为激活函数，最后一层全连接层用softmax输出10个分类。</p><h2 id="网络参数"><a href="#网络参数" class="headerlink" title="网络参数"></a>网络参数</h2><p><img src="F:\Typora图\image-20240912102620532-17261079817683.png" alt="image-20240912102620532"></p><h2 id="模型前向传播定义及简单测试"><a href="#模型前向传播定义及简单测试" class="headerlink" title="模型前向传播定义及简单测试"></a>模型前向传播定义及简单测试</h2><p>代码及详细注释如下</p><p>文件夹下新建model.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  <span class="comment"># 引入PyTorch中的神经网络模块</span></span><br><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary  <span class="comment"># 用于打印模型的摘要信息（参数数量、输入输出形状等）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义LeNet网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):  <span class="comment"># LeNet继承自nn.Module，这是PyTorch中所有神经网络模型的基类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 调用父类的构造函数，初始化模块（约定俗成）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义LeNet的卷积层和池化层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第一个卷积层：</span></span><br><span class="line">        <span class="comment"># in_channels=1：输入通道为1（适用于灰度图像，如MNIST数据集）</span></span><br><span class="line">        <span class="comment"># out_channels=6：输出通道为6，即该层提取6个特征图</span></span><br><span class="line">        <span class="comment"># kernel_size=5：5x5的卷积核</span></span><br><span class="line">        <span class="comment"># padding=2：在输入图像四周填充2个像素，使输出图像大小保持不变</span></span><br><span class="line">        <span class="comment"># stride=1：卷积核的步长为1（卷积核每次移动1个像素）</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.s1 = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 第一个池化层（平均池化层）：</span></span><br><span class="line">        <span class="comment"># kernel_size=2：池化窗口大小为2x2</span></span><br><span class="line">        <span class="comment"># stride=2：池化窗口的步长为2，即每次下采样一半的图像尺寸</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">0</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第二个卷积层：</span></span><br><span class="line">        <span class="comment"># in_channels=6：输入通道为6，即上一层的输出通道</span></span><br><span class="line">        <span class="comment"># out_channels=16：输出通道为16，即该层提取16个特征图</span></span><br><span class="line">        <span class="comment"># kernel_size=5：5x5的卷积核</span></span><br><span class="line">        <span class="comment"># padding=0：没有填充，因此输出图像尺寸会减小</span></span><br><span class="line">        <span class="comment"># stride=1：卷积核的步长为1</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.s2 = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 第二个池化层：与第一个池化层类似，2x2的池化窗口和步长为2</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.sig = nn.Sigmoid()  <span class="comment"># 激活函数：Sigmoid，用于引入非线性</span></span><br><span class="line">        <span class="comment"># 备注：现代的卷积神经网络通常使用ReLU激活函数，因为它在深层网络中表现更好</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()  <span class="comment"># 展平操作，将卷积层的多维输出展平成一维向量，以便输入全连接层</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)  </span><br><span class="line">        <span class="comment"># 全连接层1：</span></span><br><span class="line">        <span class="comment"># 输入：从卷积层展平的输出（16个5x5的特征图，因此大小是16*5*5）</span></span><br><span class="line">        <span class="comment"># 输出：120个神经元</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="comment"># 全连接层2：输入为120个神经元，输出为84个神经元</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        <span class="comment"># 全连接层3：输入为84个神经元，输出为10个神经元（10个类别的分类任务）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播过程</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)  <span class="comment"># 第一个卷积层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.sig(x)  <span class="comment"># 通过Sigmoid激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.s1(x)  <span class="comment"># 通过第一个池化层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv2(x)  <span class="comment"># 第二个卷积层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.sig(x)  <span class="comment"># 通过Sigmoid激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.s2(x)  <span class="comment"># 通过第二个池化层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)  <span class="comment"># 将多维张量展平为一维向量</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)  <span class="comment"># 通过全连接层1</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)  <span class="comment"># 通过全连接层2</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc3(x)  <span class="comment"># 通过全连接层3</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x  <span class="comment"># 返回最终的输出（10个类别的得分）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主程序</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="comment"># 检查是否有可用的GPU设备，如果有则使用CUDA，否则使用CPU</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化LeNet模型，并将其转移到设备（GPU或CPU）</span></span><br><span class="line">    model = LeNet().to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印模型的概要信息，包括每层的输入输出形状和参数数量</span></span><br><span class="line">    <span class="built_in">print</span>(summary(model, (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">    <span class="comment"># 使用summary函数显示模型结构</span></span><br><span class="line">    <span class="comment"># 输入数据的形状为(1, 28, 28)，即单通道28x28的灰度图像（典型的MNIST图像）</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="数据集介绍与展示"><a href="#数据集介绍与展示" class="headerlink" title="数据集介绍与展示"></a>数据集介绍与展示</h2><p>文件夹下新建plot.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms  <span class="comment"># 导入数据集和数据变换功能</span></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> FashionMNIST</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据的预处理变换(归一化)</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(size=<span class="number">224</span>),    <span class="comment">#图像大小设为224</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为Tensor格式</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 对图像进行标准化，使像素值在[-1, 1]之间</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载并加载FashionMNIST训练集数据</span></span><br><span class="line">train_data = FashionMNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,  <span class="comment"># 指定数据集的存储路径</span></span><br><span class="line">    train=<span class="literal">True</span>,  <span class="comment"># 加载训练集</span></span><br><span class="line">    download=<span class="literal">True</span>,  <span class="comment"># 如果数据集不存在，自动下载</span></span><br><span class="line">    transform=transform  <span class="comment"># 应用预处理操作</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载并加载FashionMNIST测试集数据</span></span><br><span class="line">test_data = FashionMNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,  <span class="comment"># 指定数据集的存储路径</span></span><br><span class="line">    train=<span class="literal">False</span>,  <span class="comment"># 加载测试集</span></span><br><span class="line">    download=<span class="literal">True</span>,  <span class="comment"># 如果数据集不存在，自动下载</span></span><br><span class="line">    transform=transform  <span class="comment"># 应用预处理操作</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用DataLoader将数据集分批加载 数据变成一捆一捆 每捆都有标签（label）</span></span><br><span class="line">train_loader = DataLoader(</span><br><span class="line">    dataset=train_data,  <span class="comment"># 传入训练数据集</span></span><br><span class="line">    batch_size=<span class="number">64</span>,  <span class="comment"># 每个批次的样本数量</span></span><br><span class="line">    shuffle=<span class="literal">True</span>  <span class="comment"># 每个epoch打乱数据</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(</span><br><span class="line">    dataset=test_data,  <span class="comment"># 传入测试数据集</span></span><br><span class="line">    batch_size=<span class="number">64</span>,  <span class="comment"># 每个批次的样本数量</span></span><br><span class="line">    shuffle=<span class="literal">False</span>  <span class="comment"># 测试集不需要打乱</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试</span></span><br><span class="line"><span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">break</span>  <span class="comment"># 只获取第一个批次的数据</span></span><br><span class="line">batch_x = b_x.squeeze().numpy()  <span class="comment"># 去掉批次中的单通道维度，并转为NumPy数组</span></span><br><span class="line">batch_y = b_y.numpy()  <span class="comment"># 将标签转为NumPy数组</span></span><br><span class="line">class_label = train_data.classes  <span class="comment"># 获取类别名称</span></span><br><span class="line"><span class="built_in">print</span>(class_label)  <span class="comment"># 打印类别标签</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试是否加载成功</span></span><br><span class="line">data_iter = <span class="built_in">iter</span>(train_loader)  <span class="comment"># 创建一个可迭代的加载器对象</span></span><br><span class="line">images, labels = <span class="built_in">next</span>(data_iter)  <span class="comment"># 从加载器中获取第一个批次的图像和标签</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Image batch dimensions: <span class="subst">&#123;images.shape&#125;</span>&quot;</span>)  <span class="comment"># 打印图像的批次维度，形状应为[batch_size, channels, height, width]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Label batch dimensions: <span class="subst">&#123;labels.shape&#125;</span>&quot;</span>)  <span class="comment"># 打印标签的批次维度，应为[batch_size]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="训练模板代码—数据加载函数"><a href="#训练模板代码—数据加载函数" class="headerlink" title="训练模板代码—数据加载函数"></a>训练模板代码—数据加载函数</h2><p>文件夹下新建train.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim  <span class="comment"># 导入PyTorch的神经网络模块和优化器模块</span></span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data  <span class="comment"># 导入数据加载相关的工具</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms  <span class="comment"># 导入数据集和数据变换功能</span></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> FashionMNIST  <span class="comment"># 引入FashionMNIST数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># 导入matplotlib用于绘图</span></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> LeNet  <span class="comment"># 假设你已经在model.py中定义了LeNet模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据预处理操作，主要包括图像调整大小和归一化</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(size=<span class="number">224</span>),  <span class="comment"># 将输入的28x28图像调整为224x224（典型用于较大的CNN模型）</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为PyTorch的Tensor格式</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 对图像进行标准化，使像素值在[-1, 1]之间</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义用于处理训练集和验证集的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_val_data_process</span>():</span><br><span class="line">    <span class="comment"># 加载FashionMNIST训练集数据</span></span><br><span class="line">    <span class="comment"># 此时train_data是完整数据集 未划分</span></span><br><span class="line">    train_data = FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>,  <span class="comment"># 数据集存储路径，如果不存在会自动创建</span></span><br><span class="line">        train=<span class="literal">True</span>,  <span class="comment"># 表示加载训练集</span></span><br><span class="line">        download=<span class="literal">True</span>,  <span class="comment"># 如果本地没有数据集，会自动从互联网下载</span></span><br><span class="line">        transform=transform  <span class="comment"># 应用定义的预处理操作</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用random_split将数据集分为80%的训练集和20%的验证集</span></span><br><span class="line">    <span class="comment"># round函数用来确保数据集大小是整数</span></span><br><span class="line">    train_data, val_data = Data.random_split(train_data, [<span class="built_in">round</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(train_data)), <span class="built_in">round</span>(<span class="number">0.2</span> * <span class="built_in">len</span>(train_data))])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建训练数据的DataLoader</span></span><br><span class="line">    train_dataloader = Data.DataLoader(</span><br><span class="line">        dataset=train_data,  <span class="comment"># 传入训练集</span></span><br><span class="line">        batch_size=<span class="number">128</span>,  <span class="comment"># 每个批次处理128个样本</span></span><br><span class="line">        shuffle=<span class="literal">True</span>,  <span class="comment"># 打乱数据，以避免训练过程中数据顺序的影响</span></span><br><span class="line">        num_workers=<span class="number">8</span>  <span class="comment"># 使用8个子进程加载数据，可以加速数据加载（Linux/Mac环境推荐）。Windows上设置为0或1</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建验证数据的DataLoader</span></span><br><span class="line">    val_dataloader = Data.DataLoader(</span><br><span class="line">        dataset=val_data,  <span class="comment"># 传入验证集</span></span><br><span class="line">        batch_size=<span class="number">128</span>,  <span class="comment"># 每个批次处理128个样本</span></span><br><span class="line">        shuffle=<span class="literal">True</span>,  <span class="comment"># 验证集也可以打乱，以提高泛化性能</span></span><br><span class="line">        num_workers=<span class="number">8</span>  <span class="comment"># 同样使用8个子进程加载数据。Windows用户请将此参数改为0或1</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_dataloader, val_dataloader  <span class="comment"># 返回训练和验证的数据加载器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化并测试数据处理函数</span></span><br><span class="line">train_val_data_process()  <span class="comment"># 运行该函数来检查代码是否正常运行</span></span><br></pre></td></tr></table></figure><h2 id="训练模板代码-初始化值定义"><a href="#训练模板代码-初始化值定义" class="headerlink" title="训练模板代码-初始化值定义"></a>训练模板代码-初始化值定义</h2><p>还是train.py的内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model_process</span>(<span class="params">model, train_dataloader, val_dataloader, num_epochs</span>):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 确定训练设备</span></span><br><span class="line">    <span class="comment"># 如果有可用的GPU（CUDA），则使用GPU，否则使用CPU</span></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 定义优化器</span></span><br><span class="line">    <span class="comment"># Adam是一种常用的优化器，适合很多类型的神经网络。这里设置了学习率为0.001。</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 定义损失函数</span></span><br><span class="line">    <span class="comment"># CrossEntropyLoss 是用于多分类问题的损失函数，特别适用于分类任务。</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 将模型放置到设备上</span></span><br><span class="line">    <span class="comment"># 将模型移动到设备上（GPU/CPU），确保模型在训练时可以利用到合适的计算资源。</span></span><br><span class="line">    model = model.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 保存模型的最佳权重</span></span><br><span class="line">    <span class="comment"># 使用 copy.deepcopy 复制模型的当前权重，以便在后面保存最优模型时使用。</span></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6. 初始化变量以跟踪最佳准确率</span></span><br><span class="line">    <span class="comment"># `best_acc` 保存训练期间模型的最佳准确率，初始值设为 0.0。</span></span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 7. 用于记录训练和验证过程中的损失值</span></span><br><span class="line">    <span class="comment"># `train_loss_all` 用于存储每个epoch的训练损失。</span></span><br><span class="line">    <span class="comment"># `val_loss_all` 用于存储每个epoch的验证损失。</span></span><br><span class="line">    train_loss_all = []</span><br><span class="line">    val_loss_all = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 8. 记录训练的起始时间</span></span><br><span class="line">    <span class="comment"># `since` 记录了训练的开始时间，以便后续计算训练的总时长。</span></span><br><span class="line">    since = time.time()</span><br></pre></td></tr></table></figure><h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><h2 id="网络参数-1"><a href="#网络参数-1" class="headerlink" title="网络参数"></a>网络参数</h2><p><img src="F:\Typora图\image-20240912102600224-17261079625181.png" alt="image-20240912102600224"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CNN经典模型LeNet实现笔记&quot;&gt;&lt;a href=&quot;#CNN经典模型LeNet实现笔记&quot; class=&quot;headerlink&quot; title=&quot;CNN经典模型LeNet实现笔记&quot;&gt;&lt;/a&gt;CNN经典模型LeNet实现笔记&lt;/h1&gt;&lt;h2 id=&quot;简介&quot;&gt;&lt;a hr</summary>
      
    
    
    
    
    <category term="CNN经典模型" scheme="https://1kunn.github.io/tags/CNN%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>论文复现_GNN+Medical</title>
    <link href="https://1kunn.github.io/2024/09/11/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0-GNN-Medical/"/>
    <id>https://1kunn.github.io/2024/09/11/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0-GNN-Medical/</id>
    <published>2024-09-10T23:25:08.000Z</published>
    <updated>2024-09-11T00:12:32.574Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>文献地址：<a href="https://ieeexplore.ieee.org/document/10484310">https://ieeexplore.ieee.org/document/10484310</a></p><p>Github地址：<a href="https://github.com/ukaukaaaa/GazeGNN?tab=readme-ov-file">https://github.com/ukaukaaaa/GazeGNN?tab=readme-ov-file</a></p><p>数据集下载：<a href="https://drive.google.com/file/d/1jB0jENWn8NqCB0w9YCuEKpgm0Uiu5fdv/view">https://drive.google.com/file/d/1jB0jENWn8NqCB0w9YCuEKpgm0Uiu5fdv/view</a></p><p>Pyramid ViG-Ti文件下载:<a href="https://github.com/huawei-noah/Efficient-AI-Backbones/releases/download/pyramid-vig/pvig_ti_78.5.pth.tar">https://github.com/huawei-noah/Efficient-AI-Backbones/releases/download/pyramid-vig/pvig_ti_78.5.pth.tar</a></p><p>源码有几处需要修改才能跑通</p><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>python&#x3D;3.9</p><p>PyTorch&#x3D;conda install pytorch&#x3D;&#x3D;2.0.0 torchvision&#x3D;&#x3D;0.15.0 torchaudio&#x3D;&#x3D;2.0.0 pytorch-cuda&#x3D;11.8 -c pytorch -c nvidia</p><p>pip install numpy matplotlib scitkit-learn tqdm pandas timm</p><h2 id="准备步骤"><a href="#准备步骤" class="headerlink" title="准备步骤"></a>准备步骤</h2><ul><li><p>从GIthub上下载代码下来</p></li><li><p>文件目录下手动新建pretrain文件夹和data文件夹，如图所示</p><p><img src="F:\Typora图\image-20240911075109999-17260122759331.png" alt="image-20240911075109999"></p></li><li><p>将Pyramid ViG-Ti文件（pvig_ti_78.5.pth.tar）放入pretrain文件夹下</p></li><li><p>将mimic_part_jpg数据集放入data文件夹下</p></li></ul><h2 id="代码修改"><a href="#代码修改" class="headerlink" title="代码修改"></a>代码修改</h2><ul><li>在<code> read_data.py</code>中的dataset里修改为绝对路径(按你自己的文件路径改)</li></ul><p><img src="F:\Typora图\image-20240911075552734-17260125547903.png" alt="image-20240911075552734"></p><p>因为绝对路径可能没有与原来的相对路径相同的目录层级，导致在提取上层目录时出现 <code>IndexError</code>。</p><p>所以：</p><p>在<code> read_data.py</code>中的getitem中修改</p><ol><li>安全标签提取：</li></ol><ul><li>使用 os.path.dirname() 和 os.path.basename() 来提取父目录名称作为标签键。</li><li>检查 label_key 是否存在于 labelsdict 中，如果不存在，则抛出一个明确的错误。</li></ul><ol start="2"><li>简化 ID 提取：</li></ol><ul><li>使用 os.path.splitext() 从图像路径中直接获取 ID，而不是使用多个字符串分割操作。</li></ul><ol start="3"><li>边界检查：</li></ol><ul><li>在填充 gaze 数组时，添加了边界检测以避免数组越界。</li></ul><p>** 此处贴上整个<code> read_data.py</code>修改后的代码*</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_mimic</span>(<span class="params">batchsize,data_dir = <span class="string">&#x27;../mimic_part_jpg&#x27;</span></span>):</span><br><span class="line">    data_transforms = &#123;</span><br><span class="line">        <span class="string">&#x27;train&#x27;</span>: transforms.Compose([</span><br><span class="line">            transforms.RandomRotation((-<span class="number">5</span>,<span class="number">5</span>)),</span><br><span class="line">            transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">            transforms.RandomHorizontalFlip(),</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            <span class="comment"># transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</span></span><br><span class="line">        ]),</span><br><span class="line">        <span class="string">&#x27;test&#x27;</span>: transforms.Compose([</span><br><span class="line">            transforms.Resize(<span class="number">256</span>),</span><br><span class="line">            transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            <span class="comment"># transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</span></span><br><span class="line">        ]),</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    image_datasets = &#123;x: dataset(mode=x, transform=data_transforms[x])</span><br><span class="line">                      <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]&#125;</span><br><span class="line">    </span><br><span class="line">    data_loader_train = DataLoader(dataset=image_datasets[<span class="string">&#x27;train&#x27;</span>],</span><br><span class="line">                                   batch_size=batchsize,</span><br><span class="line">                                   shuffle=<span class="literal">True</span>,</span><br><span class="line">                                   pin_memory=<span class="literal">True</span></span><br><span class="line">                                   )</span><br><span class="line">    data_loader_test = DataLoader(dataset=image_datasets[<span class="string">&#x27;test&#x27;</span>],</span><br><span class="line">                                  batch_size=batchsize,</span><br><span class="line">                                  shuffle=<span class="literal">False</span>,</span><br><span class="line">                                  pin_memory=<span class="literal">True</span></span><br><span class="line">                                  )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data_loader_train,data_loader_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">dataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment">#地址改成自己电脑数据集文件的绝对路径！！！</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_dir=<span class="string">r&#x27;E:\BaiduNetdiskDownload\Github_code\GazeGNN-main\data\mimic_part_jpg&#x27;</span>, mode=<span class="string">&quot;train&quot;</span>, transform=<span class="literal">None</span></span>): </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.root = data_dir</span><br><span class="line">        <span class="variable language_">self</span>.mode = mode</span><br><span class="line">        <span class="variable language_">self</span>.T = transform</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.csv = pd.read_csv(os.path.join(<span class="variable language_">self</span>.root, <span class="string">&quot;gaze&quot;</span>, <span class="string">&quot;fixations.csv&quot;</span>))</span><br><span class="line">        <span class="variable language_">self</span>.labels = [<span class="string">&quot;CHF&quot;</span>, <span class="string">&quot;Normal&quot;</span>, <span class="string">&quot;pneumonia&quot;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.labelsdict = &#123;<span class="string">&quot;CHF&quot;</span>: <span class="number">0</span>, <span class="string">&quot;Normal&quot;</span>: <span class="number">1</span>, <span class="string">&quot;pneumonia&quot;</span>: <span class="number">2</span>&#125;</span><br><span class="line">        <span class="variable language_">self</span>.idlist = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.labels)):</span><br><span class="line">            <span class="variable language_">self</span>.idlist.extend(glob(os.path.join(<span class="variable language_">self</span>.root, <span class="variable language_">self</span>.mode, <span class="variable language_">self</span>.labels[i], <span class="string">&quot;*.jpg&quot;</span>)))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.idlist)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取图像路径</span></span><br><span class="line">        imgpath = <span class="variable language_">self</span>.idlist[idx]</span><br><span class="line">        <span class="built_in">id</span> = os.path.splitext(os.path.basename(imgpath))[<span class="number">0</span>]</span><br><span class="line">        gazepath = os.path.join(<span class="variable language_">self</span>.root, <span class="string">&quot;gaze&quot;</span>, <span class="string">&quot;fixations&quot;</span>, <span class="string">f&quot;<span class="subst">&#123;<span class="built_in">id</span>&#125;</span>.npy&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取图像</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(imgpath, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            img = Image.<span class="built_in">open</span>(f)</span><br><span class="line">            img = img.convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Image path: <span class="subst">&#123;imgpath&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 安全地提取标签</span></span><br><span class="line">        parent_dir = os.path.dirname(imgpath)</span><br><span class="line">        label_key = os.path.basename(parent_dir)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> label_key <span class="keyword">in</span> <span class="variable language_">self</span>.labelsdict:</span><br><span class="line">            label = <span class="variable language_">self</span>.labelsdict[label_key]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">f&quot;Label key &#x27;<span class="subst">&#123;label_key&#125;</span>&#x27; not found in labelsdict.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取 gaze</span></span><br><span class="line">        <span class="built_in">id</span> = os.path.splitext(os.path.basename(imgpath))[<span class="number">0</span>]  <span class="comment"># 直接从文件名中提取ID</span></span><br><span class="line">        gaze = np.zeros((img.size[<span class="number">1</span>], img.size[<span class="number">0</span>]), dtype=np.float32)</span><br><span class="line">        idcsv = <span class="variable language_">self</span>.csv.loc[<span class="variable language_">self</span>.csv[<span class="string">&quot;DICOM_ID&quot;</span>] == <span class="built_in">id</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(idcsv)):</span><br><span class="line">            t = idcsv.iloc[i][<span class="string">&quot;Time (in secs)&quot;</span>] - (idcsv.iloc[i - <span class="number">1</span>][<span class="string">&quot;Time (in secs)&quot;</span>] <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">            x = <span class="built_in">int</span>(idcsv.iloc[i][<span class="string">&quot;X_ORIGINAL&quot;</span>])</span><br><span class="line">            y = <span class="built_in">int</span>(idcsv.iloc[i][<span class="string">&quot;Y_ORIGINAL&quot;</span>])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; gaze.shape[<span class="number">1</span>] <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; gaze.shape[<span class="number">0</span>]:</span><br><span class="line">                gaze[y, x] = t</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 归一化和转换为图像</span></span><br><span class="line">        gaze = np.log(gaze + <span class="number">0.01</span>)</span><br><span class="line">        gaze = (((gaze - gaze.<span class="built_in">min</span>()) / (gaze.<span class="built_in">max</span>() - gaze.<span class="built_in">min</span>())) * <span class="number">255</span>).astype(np.uint8)</span><br><span class="line">        gimg = gaze[..., np.newaxis].repeat(<span class="number">3</span>, axis=<span class="number">2</span>)</span><br><span class="line">        gimg = Image.fromarray(gimg)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 图像变换</span></span><br><span class="line">        state = torch.get_rng_state()</span><br><span class="line">        img = <span class="variable language_">self</span>.T(img)</span><br><span class="line">        img = transforms.functional.normalize(img, [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line">        torch.set_rng_state(state)</span><br><span class="line">        gaze = <span class="variable language_">self</span>.T(gimg)</span><br><span class="line">        gaze = <span class="variable language_">self</span>.getPatchGaze(gaze[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, label, gaze</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getPatchGaze</span>(<span class="params">self, gaze</span>):</span><br><span class="line">        g = np.zeros((<span class="number">56</span>,<span class="number">56</span>), dtype=np.float32)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">56</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">56</span>):</span><br><span class="line">                x1 = <span class="number">4</span>*i-<span class="number">7</span></span><br><span class="line">                x2 = <span class="number">4</span>*i+<span class="number">7</span></span><br><span class="line">                y1 = <span class="number">4</span>*j-<span class="number">7</span></span><br><span class="line">                y2 = <span class="number">4</span>*j+<span class="number">7</span></span><br><span class="line">                <span class="keyword">if</span> x1 &lt; <span class="number">0</span>:</span><br><span class="line">                    x1 = <span class="number">0</span></span><br><span class="line">                <span class="keyword">if</span> y1 &lt; <span class="number">0</span>:</span><br><span class="line">                    y1 = <span class="number">0</span></span><br><span class="line">                <span class="keyword">if</span> x2 &gt; <span class="number">223</span>:</span><br><span class="line">                    x2 = <span class="number">223</span></span><br><span class="line">                <span class="keyword">if</span> y2 &gt; <span class="number">223</span>:</span><br><span class="line">                    y2 = <span class="number">223</span></span><br><span class="line">                g[i,j] = gaze[x1:x2, y1:y2].<span class="built_in">sum</span>()</span><br><span class="line">        <span class="keyword">if</span> g.<span class="built_in">max</span>()-g.<span class="built_in">min</span>() != <span class="number">0</span>:</span><br><span class="line">            g = (g-g.<span class="built_in">min</span>())/(g.<span class="built_in">max</span>()-g.<span class="built_in">min</span>())</span><br><span class="line">        <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="跑通结果"><a href="#跑通结果" class="headerlink" title="跑通结果"></a>跑通结果</h2><p>在控制台输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py</span><br></pre></td></tr></table></figure><p><img src="F:\Typora图\image-20240911080438769-17260130817625.png" alt="image-20240911080438769"></p><p><img src="F:\Typora图\image-20240911080452038-17260130932417.png" alt="image-20240911080452038"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;文献地址：&lt;a href=&quot;https://ieeexplore.ieee.org/document/10484310&quot;&gt;https://i</summary>
      
    
    
    
    
    <category term="代码复现" scheme="https://1kunn.github.io/tags/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch教程</title>
    <link href="https://1kunn.github.io/2024/09/07/PyTorch%E6%95%99%E7%A8%8B/"/>
    <id>https://1kunn.github.io/2024/09/07/PyTorch%E6%95%99%E7%A8%8B/</id>
    <published>2024-09-07T02:56:14.000Z</published>
    <updated>2024-09-09T10:41:52.285Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch教程笔记"><a href="#PyTorch教程笔记" class="headerlink" title="PyTorch教程笔记"></a>PyTorch教程笔记</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><a href="https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html">Tensors</a> || <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">Datasets &amp; DataLoaders</a> || <a href="https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html">Transforms</a> || <a href="https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html">Build Model</a> || <a href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">Autograd</a> || <a href="https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html">Optimization</a> || <a href="https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html">Save &amp; Load Model</a></p><p>视频地址：<a href="https://www.bilibili.com/video/BV1ov411M7xL/?spm_id_from=333.999.0.0&vd_source=49378ece86ab83ee76bcaac1cc96562c">https://www.bilibili.com/video/BV1ov411M7xL/?spm_id_from=333.999.0.0&amp;vd_source=49378ece86ab83ee76bcaac1cc96562c</a></p><p>环境：python3.9</p><p>​PyTorch&#x3D;conda install pytorch&#x3D;&#x3D;2.0.0 torchvision&#x3D;&#x3D;0.15.0 torchaudio&#x3D;&#x3D;2.0.0 pytorch-cuda&#x3D;11.8 -c pytorch -c nvidia</p><p>官方文档 ：<a href="https://pytorch.org/tutorials/beginner/basics/intro.html">https://pytorch.org/tutorials/beginner/basics/intro.html</a></p><h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><p>张量是一种特殊的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量来编码模型的输入和输出以及模型的参数。</p><h3 id="张量的创建"><a href="#张量的创建" class="headerlink" title="张量的创建"></a>张量的创建</h3><p>张量可以通过多种方式初始化。请看以下示例：</p><h4 id="直接来自数据"><a href="#直接来自数据" class="headerlink" title="直接来自数据"></a>直接来自数据</h4><p>张量可以直接从数据创建。数据类型会自动推断。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure><h4 id="来自NumPy张量"><a href="#来自NumPy张量" class="headerlink" title="来自NumPy张量"></a>来自NumPy张量</h4><p>可以从 NumPy 数组创建张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br></pre></td></tr></table></figure><h4 id="从另一个张量："><a href="#从另一个张量：" class="headerlink" title="从另一个张量："></a><strong>从另一个张量：</strong></h4><p>除非明确覆盖，否则新的张量将保留参数张量的属性（形状、数据类型）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># 创建一个全1的张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line">x_rand = torch.rand_like(x_data, dtype=torch.<span class="built_in">float</span>) <span class="comment"># 创建一个随机数的张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">Random Tensor:</span><br><span class="line"> tensor([[<span class="number">0.4223</span>, <span class="number">0.1719</span>],</span><br><span class="line">        [<span class="number">0.3184</span>, <span class="number">0.2631</span>]])</span><br></pre></td></tr></table></figure><p><strong>使用随机值或常数值：</strong></p><p><code>shape</code>是张量维度的元组。在下面的函数中，它决定了输出张量的维数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">shape = (<span class="number">2</span>,<span class="number">3</span>,) <span class="comment">#2行3列的形状</span></span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Zeros Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>上面的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Random Tensor:</span><br><span class="line"> tensor([[<span class="number">0.3904</span>, <span class="number">0.6009</span>, <span class="number">0.2566</span>],</span><br><span class="line">        [<span class="number">0.7936</span>, <span class="number">0.9408</span>, <span class="number">0.1332</span>]])</span><br><span class="line"></span><br><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">Zeros Tensor:</span><br><span class="line"> tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><h2 id="张量的属性"><a href="#张量的属性" class="headerlink" title="张量的属性"></a>张量的属性</h2><p>张量属性描述它们的形状、数据类型和存储它们的设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Shape of tensor: torch.Size([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Datatype of tensor: torch.float32</span><br><span class="line">Device tensor <span class="keyword">is</span> stored on: cpu</span><br></pre></td></tr></table></figure><p><strong>常用运算</strong></p><p>待续</p><h2 id="数据集Dataset和数据加载器DataLoader"><a href="#数据集Dataset和数据加载器DataLoader" class="headerlink" title="数据集Dataset和数据加载器DataLoader"></a>数据集Dataset和数据加载器DataLoader</h2><p>处理数据样本的代码可能会变得混乱且难以维护；理想情况下，我们希望将数据集代码与模型训练代码分离，以提高可读性和模块化。PyTorch 提供了两个数据原语：<code>torch.utils.data.DataLoader</code>和<code>torch.utils.data.Dataset</code> ，允许您使用预加载的数据集以及您自己的数据。 <code>Dataset</code>存储样本及其相应的标签，并<code>DataLoader</code>使用可迭代对象包装样本，<code>Dataset</code>以便轻松访问样本。</p><p>PyTorch 域库提供了许多预加载的数据集（例如 FashionMNIST），这些数据集<code>torch.utils.data.Dataset</code>可对特定数据进行子类化和实现特定函数。</p><h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><p>以下是如何从 TorchVision 加载<a href="https://research.zalando.com/project/fashion_mnist/fashion_mnist/">Fashion-MNIST</a>数据集的示例。Fashion-MNIST 是 Zalando 文章图像的数据集，包含 60,000 个训练示例和 10,000 个测试示例。每个示例包含一个 28×28 灰度图像和一个来自 10 个类别之一的相关标签。</p><ul><li><p>我们使用以下参数加载<a href="https://pytorch.org/vision/stable/datasets.html#fashion-mnist">FashionMNIST数据集：</a></p></li><li><p><code>root</code>是存储训练&#x2F;测试数据的路径，</p></li><li><p><code>train</code>指定训练或测试数据集，</p></li><li><p><code>download=True</code>如果 上没有数据，则从互联网上下载数据<code>root</code>。</p></li><li><p><code>transform</code>并<code>target_transform</code>指定特征和标签转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,<span class="comment">#下载路径是本地目录下的data文件夹</span></span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,<span class="comment">#没有文件的话就下载</span></span><br><span class="line">    transform=ToTensor() <span class="comment">#对数据进行预处理，这里使用 ToTensor() 将图像转换为张量格式，以便于后续的深度学习模型处理</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="迭代并可视化数据集"><a href="#迭代并可视化数据集" class="headerlink" title="迭代并可视化数据集"></a>迭代并可视化数据集</h2><p>我们可以<code>Datasets</code>像列表一样手动索引：<code>training_data[index]</code>。我们用<code>matplotlib</code>它来可视化训练数据中的某些样本。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个字典，将标签映射到对应的衣物类别</span></span><br><span class="line">labels_map = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;T-Shirt&quot;</span>,         <span class="comment"># 标签 0 对应 T-Shirt</span></span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;Trouser&quot;</span>,        <span class="comment"># 标签 1 对应 Trouser</span></span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;Pullover&quot;</span>,       <span class="comment"># 标签 2 对应 Pullover</span></span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;Dress&quot;</span>,          <span class="comment"># 标签 3 对应 Dress</span></span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;Coat&quot;</span>,           <span class="comment"># 标签 4 对应 Coat</span></span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;Sandal&quot;</span>,         <span class="comment"># 标签 5 对应 Sandal</span></span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;Shirt&quot;</span>,          <span class="comment"># 标签 6 对应 Shirt</span></span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;Sneaker&quot;</span>,        <span class="comment"># 标签 7 对应 Sneaker</span></span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;Bag&quot;</span>,            <span class="comment"># 标签 8 对应 Bag</span></span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;Ankle Boot&quot;</span>,     <span class="comment"># 标签 9 对应 Ankle Boot</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个图形，设置大小为 8x8 英寸</span></span><br><span class="line">figure = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义列数和行数</span></span><br><span class="line">cols, rows = <span class="number">3</span>, <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环生成 9 张样本图像</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, cols * rows + <span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 利用 PyTorch 的 randint 函数生成一个随机索引，范围是在 training_data 的长度之间。size=(1,) 表示生成一个随机数，.item() 用于提取出这个值。</span></span><br><span class="line">    sample_idx = torch.randint(<span class="built_in">len</span>(training_data), size=(<span class="number">1</span>,)).item()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从训练数据中获取随机样本的图像和标签</span></span><br><span class="line">    img, label = training_data[sample_idx]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在图形中添加一个子图</span></span><br><span class="line">    figure.add_subplot(rows, cols, i)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置子图的标题为对应的标签名称</span></span><br><span class="line">    plt.title(labels_map[label])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 关闭坐标轴显示</span></span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 显示图像，使用灰度色彩映射</span></span><br><span class="line">    plt.imshow(img.squeeze(), cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示最终的图形</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>创建自定义数据集</strong></p><p>自定义 Dataset 类必须实现三个函数：__init__、__len__和__getitem__。看一下这个实现；FashionMNIST 图像存储在目录中<code>img_dir</code>，其标签分别存储在 CSV 文件中<code>annotations_file</code>。</p><p>在接下来的部分中，我们将详细分析每个功能中发生的情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os  <span class="comment"># 导入操作系统模块，用于处理文件和目录</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  <span class="comment"># 导入 pandas 库，用于数据处理和数据框操作</span></span><br><span class="line"><span class="keyword">from</span> torchvision.io <span class="keyword">import</span> read_image  <span class="comment"># 从 torchvision 库导入 read_image 函数，用于读取图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集类 CustomImageDataset，继承自 Dataset 类（来自 PyTorch）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomImageDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="comment"># 初始化方法，用于设置数据集的基本属性</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 读取包含图像路径和标签的 CSV 文件，返回一个 DataFrame</span></span><br><span class="line">        <span class="variable language_">self</span>.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 存储图像所在目录的路径</span></span><br><span class="line">        <span class="variable language_">self</span>.img_dir = img_dir</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 可选的转换函数，用于对图像进行预处理</span></span><br><span class="line">        <span class="variable language_">self</span>.transform = transform</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 可选的转换函数，用于对标签进行处理</span></span><br><span class="line">        <span class="variable language_">self</span>.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回数据集中样本的数量</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.img_labels)  <span class="comment"># 数据集的长度等于图像标签的数量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据索引获取特定样本</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 根据索引构建图像的完整路径</span></span><br><span class="line">        img_path = os.path.join(<span class="variable language_">self</span>.img_dir, <span class="variable language_">self</span>.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        <span class="comment">#iloc通过整数位置选择数据</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 使用 read_image 函数读取图像，将图像数据加载到内存中</span></span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获取对应的标签</span></span><br><span class="line">        label = <span class="variable language_">self</span>.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果提供了图像变换函数，则应用它</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.transform:</span><br><span class="line">            image = <span class="variable language_">self</span>.transform(image)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果提供了标签变换函数，则应用它</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.target_transform:</span><br><span class="line">            label = <span class="variable language_">self</span>.target_transform(label)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 返回处理后的图像和标签</span></span><br><span class="line">        <span class="keyword">return</span> image, label</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="init"><a href="#init" class="headerlink" title="__init__"></a><code>__init__</code></h3><ul><li>接收参数：<code>annotations_file</code> 是包含图像路径和标签的 CSV 文件，<code>img_dir</code> 是图像所在目录，<code>transform</code> 和 <code>target_transform</code> 是用于数据增强或预处理的可选参数。</li><li>使用 <code>pd.read_csv</code> 读取 CSV 文件，并将结果存储在 <code>self.img_labels</code> 中。</li><li>存储图像目录路径到 <code>self.img_dir</code>。</li><li>将变换函数分别存储到实例变量中，以便后续使用。</li></ul><h3 id="len"><a href="#len" class="headerlink" title="__len__"></a><code>__len__</code></h3><ul><li>返回数据集中样本的数量，即图像标签的数量。这使得其他部分的代码可以通过 <code>len(dataset)</code> 获取数据集的大小。</li></ul><h3 id="getitem"><a href="#getitem" class="headerlink" title="__getitem__"></a><code>__getitem__</code></h3><ul><li>接收一个索引 <code>idx</code>，根据该索引返回对应的图像和标签。</li><li>使用 <code>os.path.join</code> 构建图像的完整路径。</li><li>使用 <code>read_image</code> 加载图像数据，这里读取的图像是以张量形式返回的。</li><li>获取图像的标签，通过 <code>iloc</code> 方法从 DataFrame 中提取。</li><li>检查是否提供了图像变换函数，如果有，则对图像应用该变换。</li><li>同样地，检查是否提供了标签变换函数，并应用它。</li><li>最后，返回处理后的图像和标签。</li></ul><h2 id="使用-DataLoaders-准备训练数据"><a href="#使用-DataLoaders-准备训练数据" class="headerlink" title="使用 DataLoaders 准备训练数据"></a>使用 DataLoaders 准备训练数据</h2><p>检索<code>Dataset</code>数据集的特征并一次标记一个样本。在训练模型时，我们通常希望以“小批量”传递样本，在每个时期重新调整数据以减少模型过度拟合，并使用 Python<code>multiprocessing</code>来加速数据检索。</p><p><code>DataLoader</code>是一个可迭代对象，它通过一个简单的 API 为我们抽象了这种复杂性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)<span class="comment">#训练数据集</span></span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)<span class="comment">#测试数据集</span></span><br></pre></td></tr></table></figure><p><strong><code>batch_size=64</code></strong>:</p><ul><li><code>batch_size</code> 是每个批次中样本的数量。在这个例子中，每个批次将包含 64 个样本。这使得模型可以在处理整个数据集时不会占用过多的内存。</li></ul><p><strong><code>shuffle=True</code></strong>:</p><ul><li>当 <code>shuffle</code> 设置为 <code>True</code> 时，在每个 epoch 结束后会随机打乱数据，使得模型能够更好地泛化，避免学习到特定的顺序或模式</li></ul><p><strong>遍历数据集</strong></p><p>我们已将该数据集加载到中，<code>DataLoader</code>并且可以根据需要迭代数据集。下面的每次迭代都会返回一批<code>train_features</code>和<code>train_labels</code>（<code>batch_size=64</code>分别包含特征和标签）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取一个批次的特征和标签</span></span><br><span class="line">train_features, train_labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印特征和标签的形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Feature batch shape: <span class="subst">&#123;train_features.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Labels batch shape: <span class="subst">&#123;train_labels.size()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第一幅图像和标签</span></span><br><span class="line">img = train_features[<span class="number">0</span>].squeeze()</span><br><span class="line">label = train_labels[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line">plt.imshow(img, cmap=<span class="string">&quot;gray&quot;</span>) <span class="comment">#cmap=&quot;gray&quot; 指定使用灰度色图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印标签</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><strong><code>iter(train_dataloader)</code></strong>: 创建一个迭代器，用于遍历 <code>train_dataloader</code>。</li><li><strong><code>next(...)</code></strong>: 从这个迭代器中获取下一个批次的特征（<code>train_features</code>）和标签（<code>train_labels</code>）。</li></ul><h1 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h1><p>数据并不总是以训练机器学习算法所需的最终处理形式出现。我们使用<strong>转换</strong>来对数据进行一些操作，使其适合训练。</p><p>所有 TorchVision 数据集都有两个参数 -<code>transform</code>修改特征和 <code>target_transform</code>修改标签 - 它们接受包含转换逻辑的可调用函数。torchvision.transforms<a href="https://pytorch.org/vision/stable/transforms.html">模块</a>提供了几种常用的开箱即用转换。</p><p>FashionMNIST 特征采用 PIL 图像格式，标签为整数。为了进行训练，我们需要将特征作为归一化张量，将标签作为独热编码张量。为了进行这些转换，我们使用<code>ToTensor</code>和<code>Lambda</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">ds = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment">#将输入图像转换为 PyTorch 的 Tensor 格式,转换后，图像的形状将变为 (channels, height, width)，灰度图像将具有 1 个通道。</span></span><br><span class="line">    transform=ToTensor(),</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#创建一个大小为 10 的零张量（Fashion-MNIST 有 10 类）。</span></span><br><span class="line">    <span class="comment">#使用 scatter_ 方法将对应类别的值设置为 1，形成 one-hot 编码。例如，如果标签是 2，结果将是 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</span></span><br><span class="line">    target_transform=Lambda(<span class="keyword">lambda</span> y: torch.zeros(<span class="number">10</span>, dtype=torch.<span class="built_in">float</span>).scatter_(<span class="number">0</span>, torch.tensor(y), value=<span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>ToTensor()</strong></p><p><a href="https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor">ToTensor</a> 将 PIL 图像或 NumPy 转换<code>ndarray</code>为<code>FloatTensor</code>. 并将图像的像素强度值缩放到范围 [0., 1.] 内</p><p><strong>Lambda Transforms</strong></p><p>Lambda 变换应用任何用户定义的 lambda 函数.</p><h1 id="建立神经网络"><a href="#建立神经网络" class="headerlink" title="建立神经网络"></a>建立神经网络</h1><p>神经网络由对数据执行操作的层&#x2F;模块组成。torch.nn命名空间提供了构建自己的神经网络所需的所有构建块。PyTorch 中的每个模块都是nn.Module的子类。神经网络本身就是一个由其他模块（层）组成的模块。这种嵌套结构允许轻松构建和管理复杂的架构。</p><p>以下构建一个神经网络来对 FashionMNIST 数据集中的图像进行分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br></pre></td></tr></table></figure><p><strong>获取训练设备</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>) <span class="comment">#看设备</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using <span class="subst">&#123;device&#125;</span> device&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>定义类</strong></p><p>我们通过子类化来定义我们的神经网络<code>nn.Module</code>，并在中初始化神经网络层<code>__init__</code>。每个<code>nn.Module</code>子类都在方法中实现对输入数据的操作<code>forward</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 调用父类的初始化方法</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义一个展平层，用于将输入图像展平为一维</span></span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义一个包含多个全连接层和 ReLU 激活函数的序列</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),  <span class="comment"># 第一个全连接层，将784个输入映射到512个神经元</span></span><br><span class="line">            nn.ReLU(),              <span class="comment"># 应用ReLU激活函数</span></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),    <span class="comment"># 第二个全连接层，维持512个神经元</span></span><br><span class="line">            nn.ReLU(),              <span class="comment"># 再次应用ReLU激活函数</span></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),     <span class="comment"># 输出层，将512个神经元映射到10个输出类别</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 前向传播方法</span></span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)               <span class="comment"># 将输入展平为一维</span></span><br><span class="line">        logits = <span class="variable language_">self</span>.linear_relu_stack(x)  <span class="comment"># 通过全连接层序列计算输出</span></span><br><span class="line">        <span class="keyword">return</span> logits                      <span class="comment"># 返回未经过softmax的原始分数（logits）</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p><strong><code>super().__init__()</code></strong>: 确保父类 <code>nn.Module</code> 的初始化被调用，以适当地设置模块。</p></li><li><p><strong><code>self.flatten = nn.Flatten()</code></strong>: 定义展平层，使得输入形状变为一维张量，便于后续处理。</p></li><li><p><strong><code>nn.Sequential(...)</code></strong>: 按顺序组合多个层，形成网络的主架构。</p></li><li><p><strong><code>nn.Linear(...)</code></strong>: 创建全连接层。第一个参数是输入特征的数量，第二个参数是输出特征的数量。</p></li><li><p><strong><code>nn.ReLU()</code></strong>: 应用 ReLU 激活函数，可以引入非线性。</p></li><li><p><strong><code>def forward(self, x):</code></strong>: 定义网络的前向传播过程，接收输入并返回输出。</p></li><li><p><strong><code>return logits</code></strong>: 返回通过网络计算得到的原始预测值，通常在分类任务中会经过 softmax 函数转化为概率分布</p></li></ul><p><strong>我们创建 的一个实例<code>NeuralNetwork</code>，并将其移动到<code>device</code></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralNetwork().to(device)</span><br></pre></td></tr></table></figure><p>要使用模型，我们需要将输入数据传递给它。这将执行模型的<code>forward</code>以及一些<a href="https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866">后台操作</a>。请勿<code>model.forward()</code>直接调用！</p><p>在输入上调用模型会返回一个二维张量，其中 dim&#x3D;0 对应于每个类的 10 个原始预测值的每个输出，dim&#x3D;1 对应于每个输出的单个值。我们通过将其传递给模块的一个实例来获得预测概率<code>nn.Softmax</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机生成一个形状为 (1, 28, 28) 的张量，代表一张 28x28 的图像。</span></span><br><span class="line"><span class="comment"># `device` 是指定设备（比如 CPU 或 GPU）。</span></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输入传递给模型，获得 logits（未经过 softmax 的输出）。</span></span><br><span class="line">logits = model(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 Softmax 函数将 logits 转换为概率分布，dim=1 指定在类别维度上进行计算。</span></span><br><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取概率分布中最大概率的索引，即预测的类标签。</span></span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印预测的类别。</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="自动微分torch-autograd"><a href="#自动微分torch-autograd" class="headerlink" title="自动微分torch.autograd"></a>自动微分<code>torch.autograd</code></h1><p>在训练神经网络时，最常用的算法是 <strong>反向传播</strong>。在该算法中，根据损失函数关于给定参数的<strong>梯度</strong>来调整参数（模型权重） 。</p><p>为了计算这些梯度，PyTorch 有一个内置的微分引擎，称为<code>torch.autograd</code>。它支持自动计算任何计算图的梯度。</p><p>考虑最简单的单层神经网络，具有输入<code>x</code>、参数<code>w</code>和<code>b</code>以及一些损失函数。它可以在 PyTorch 中按以下方式定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为 (5,) 的张量 x，所有元素为 1，作为输入。</span></span><br><span class="line">x = torch.ones(<span class="number">5</span>)  <span class="comment"># input tensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为 (3,) 的张量 y，所有元素为 0，作为期望的输出。</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>)  <span class="comment"># expected output</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为 (5, 3) 的张量 w，带有随机初始化的权重，要求计算梯度。</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为 (3,) 的张量 b，带有随机初始化的偏置，要求计算梯度。</span></span><br><span class="line">b = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算线性变换 z = x * w + b</span></span><br><span class="line">z = torch.matmul(x, w) + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算二元交叉熵损失，使用 logits 和期望输出 y</span></span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算梯度</span></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure><h1 id="优化模型参数"><a href="#优化模型参数" class="headerlink" title="优化模型参数"></a>优化模型参数</h1><p>训练模型是一个迭代过程；在每次迭代中，模型都会对输出进行猜测，计算猜测中的误差（<em>损失</em>），收集误差相对于其参数的导数，并使用梯度下降<strong>优化这些参数。</strong></p><p><strong>先决条件代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据集 FashionMNIST，并将其转换为 Tensor 格式。</span></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载测试数据集 FashionMNIST，并将其转换为 Tensor 格式。</span></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练数据的 DataLoader，用于分批次加载数据，batch_size 设置为 64。</span></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建测试数据的 DataLoader，用于分批次加载数据，batch_size 设置为 64。</span></span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个神经网络类 NeuralNetwork，继承自 nn.Module。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 将输入张量展平为一维。</span></span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="comment"># 定义一个顺序容器，包含多个全连接层和 ReLU 激活函数。</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),  <span class="comment"># 输入层到隐藏层</span></span><br><span class="line">            nn.ReLU(),               <span class="comment"># ReLU 激活函数</span></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),     <span class="comment"># 隐藏层到隐藏层</span></span><br><span class="line">            nn.ReLU(),               <span class="comment"># ReLU 激活函数</span></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),      <span class="comment"># 隐藏层到输出层（10 个类别）</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播方法，定义数据如何通过网络流动。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)                          <span class="comment"># 展平输入</span></span><br><span class="line">        logits = <span class="variable language_">self</span>.linear_relu_stack(x)          <span class="comment"># 通过线性-激活层</span></span><br><span class="line">        <span class="keyword">return</span> logits                                <span class="comment"># 返回 logits 输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化神经网络模型</span></span><br><span class="line">model = NeuralNetwork()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>超参数</strong></p><p>超参数是可调整的参数，可控制模型优化过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">1e-3</span> <span class="comment">#学习率</span></span><br><span class="line">batch_size = <span class="number">64</span><span class="comment">#批次大小</span></span><br><span class="line">epochs = <span class="number">5</span><span class="comment">#迭代次数</span></span><br></pre></td></tr></table></figure><p><strong>损失函数</strong></p><p>当提供一些训练数据时，未经训练的网络很可能不会给出正确答案。<strong>损失函数</strong>测量所得结果与目标值的差异程度，而这正是我们在训练期间想要最小化的损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实数据标签值进行比较。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><p><strong>优化器</strong></p><p>优化是调整模型参数以减少每个训练步骤中的模型误差的过程。所有优化逻辑都封装在对象中<code>optimizer</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义学习率（可以根据需要调整）</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器为随机梯度下降（SGD），并传入模型参数以及学习率</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>上面内容示例代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数设置</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集和数据加载器</span></span><br><span class="line">transform = transforms.ToTensor()</span><br><span class="line">train_data = datasets.FashionMNIST(root=<span class="string">&#x27;data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = NeuralNetwork()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br><span class="line">        <span class="comment"># 前向传播：计算预测结果</span></span><br><span class="line">        pred = model(X)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = loss_fn(pred, y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播和优化</span></span><br><span class="line">        optimizer.zero_grad()   <span class="comment"># 清空之前的梯度</span></span><br><span class="line">        loss.backward()         <span class="comment"># 计算当前梯度</span></span><br><span class="line">        optimizer.step()        <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="保存并加载模型"><a href="#保存并加载模型" class="headerlink" title="保存并加载模型"></a>保存并加载模型</h1><p><strong>保存和加载模型权重</strong></p><p>PyTorch 模型将学习到的参数存储在名为 的内部状态字典中<code>state_dict</code>。这些可以通过以下<code>torch.save</code> 方法持久化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载一个 VGG16 模型，该模型在 ImageNet 数据集上进行了预训练。weights=&#x27;IMAGENET1K_V1&#x27; 表示使用的是版本 1 的 ImageNet 权重。</span></span><br><span class="line">model = models.vgg16(weights=<span class="string">&#x27;IMAGENET1K_V1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型的参数（权重）到一个名为 model_weights.pth 的文件中。state_dict() 返回一个包含所有可学习参数的字典。</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model_weights.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个未经过训练的 VGG16 模型</span></span><br><span class="line">model = models.vgg16()  <span class="comment"># 不指定 weights，创建未训练的模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载保存的权重</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model_weights.pth&#x27;</span>, weights_only=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型设置为评估模式，可以得到更稳定的输出。</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;PyTorch教程笔记&quot;&gt;&lt;a href=&quot;#PyTorch教程笔记&quot; class=&quot;headerlink&quot; title=&quot;PyTorch教程笔记&quot;&gt;&lt;/a&gt;PyTorch教程笔记&lt;/h1&gt;&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;head</summary>
      
    
    
    
    
    <category term="理论" scheme="https://1kunn.github.io/tags/%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>Unet手写代码过程</title>
    <link href="https://1kunn.github.io/2024/09/04/Unet%E6%89%8B%E5%86%99%E4%BB%A3%E7%A0%81%E8%BF%87%E7%A8%8B/"/>
    <id>https://1kunn.github.io/2024/09/04/Unet%E6%89%8B%E5%86%99%E4%BB%A3%E7%A0%81%E8%BF%87%E7%A8%8B/</id>
    <published>2024-09-04T03:58:30.000Z</published>
    <updated>2024-09-05T01:58:11.417Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Unet手写代码笔记"><a href="#Unet手写代码笔记" class="headerlink" title="Unet手写代码笔记"></a>Unet手写代码笔记</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>视频教程：<a href="https://www.bilibili.com/video/BV11341127iK/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=49378ece86ab83ee76bcaac1cc96562c">https://www.bilibili.com/video/BV11341127iK/?spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=49378ece86ab83ee76bcaac1cc96562c</a></p><p>数据集下载：<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar</a></p><p>源码：<a href="https://github.com/qiaofengsheng/pytorch-UNet.git">https://github.com/qiaofengsheng/pytorch-UNet.git</a></p><h2 id="什么是Unet"><a href="#什么是Unet" class="headerlink" title="什么是Unet"></a>什么是Unet</h2><p>语义分割算法。</p><p>将图片通过卷积操作缩小，再通过池化层（反卷积）一步步复原成图片原来的样子，去将图像分割。</p><p><img src="F:\Typora图\image-20240904120212599-17254225339551.png" alt="image-20240904120212599"></p><h2 id="数据集制作"><a href="#数据集制作" class="headerlink" title="数据集制作"></a>数据集制作</h2><p><strong>文件下的data和utils.py文件</strong></p><p><u>data.py代码</u></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])      <span class="comment">#图片归一化</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#处理数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, path</span>):</span><br><span class="line">        <span class="variable language_">self</span>.path = path        <span class="comment">#先获取地址</span></span><br><span class="line">        <span class="variable language_">self</span>.name = os.listdir(os.path.join(path, <span class="string">&#x27;SegmentationClass&#x27;</span>))     <span class="comment">#listdir获取目录下所有文件名</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.name)   <span class="comment">#文件名的数量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#数据追踪</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        segment_name = <span class="variable language_">self</span>.name[index]  <span class="comment"># 获取下标。 图片是xx.png，但原图是jpg所以要转化</span></span><br><span class="line">        segment_path = os.path.join(<span class="variable language_">self</span>.path, <span class="string">&#x27;SegmentationClass&#x27;</span>, segment_name)   <span class="comment">#拼接地址 数据为 原始路径、标签、下标</span></span><br><span class="line">        image_path = os.path.join(<span class="variable language_">self</span>.path, <span class="string">&#x27;JPEGImages&#x27;</span>, segment_name)        <span class="comment">#同上拼接</span></span><br><span class="line">        segment_image = keep_image_size_open(segment_path)      <span class="comment">#使用utils中的图片改造（等比缩放）</span></span><br><span class="line">        image = keep_image_size_open_rgb(image_path)</span><br><span class="line">        <span class="keyword">return</span> transform(image), torch.Tensor(np.array(segment_image))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#测试图片数据有没有问题</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> one_hot</span><br><span class="line">    data = MyDataset(<span class="string">&#x27;data&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(data[<span class="number">0</span>][<span class="number">0</span>].shape)</span><br><span class="line">    <span class="built_in">print</span>(data[<span class="number">0</span>][<span class="number">1</span>].shape)</span><br><span class="line">    out=one_hot(data[<span class="number">0</span>][<span class="number">1</span>].long())</span><br><span class="line">    <span class="built_in">print</span>(out.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在class MyDataset中</p><ul><li><p>self.path地址仅获取到此</p><p><img src="F:\Typora图\image-20240905075148454-17254939099783.png" alt="image-20240905075148454"></p></li><li><p>所以self.name里要用join合并地址</p></li></ul><p><u>utils.py代码</u></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment">#等比缩放图片</span></span><br><span class="line"><span class="comment">#用图片最长边设置为一个新矩形 将图片放入再等比缩放</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">keep_image_size_open</span>(<span class="params">path, size=(<span class="params"><span class="number">256</span>, <span class="number">256</span></span>)</span>):    <span class="comment">#传入地址 图片大小设置</span></span><br><span class="line">    img = Image.<span class="built_in">open</span>(path)      <span class="comment">#读入</span></span><br><span class="line">    temp = <span class="built_in">max</span>(img.size)        <span class="comment">#获取最长边</span></span><br><span class="line">    mask = Image.new(<span class="string">&#x27;P&#x27;</span>, (temp, temp))     <span class="comment">#设一个新矩阵</span></span><br><span class="line">    mask.paste(img, (<span class="number">0</span>, <span class="number">0</span>))     <span class="comment">#把原图粘贴</span></span><br><span class="line">    mask = mask.resize(size)        <span class="comment">#等比缩放</span></span><br><span class="line">    <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">keep_image_size_open_rgb</span>(<span class="params">path, size=(<span class="params"><span class="number">256</span>, <span class="number">256</span></span>)</span>):</span><br><span class="line">    img = Image.<span class="built_in">open</span>(path)</span><br><span class="line">    temp = <span class="built_in">max</span>(img.size)</span><br><span class="line">    mask = Image.new(<span class="string">&#x27;RGB&#x27;</span>, (temp, temp))</span><br><span class="line">    mask.paste(img, (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    mask = mask.resize(size)</span><br><span class="line">    <span class="keyword">return</span> mask</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Unet网络结构"><a href="#Unet网络结构" class="headerlink" title="Unet网络结构"></a>Unet网络结构</h2><p>conv：卷积</p><p>max pool：下采样</p><p>up-conv：上采样</p><p><img src="F:\Typora图\image-20240905082058337-17254956614435.png" alt="image-20240905082058337"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#卷积板块</span></span><br><span class="line"><span class="comment">#两个3×3卷积层</span></span><br><span class="line"><span class="comment">#收缩路径和扩张路径中的每一步都有两个3×3卷积层，然后是 RelU 激活。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv_Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channel,out_channel</span>):      <span class="comment">#不同位置的输入输出通道不一样 不能定死 所以设置in out channel</span></span><br><span class="line">        <span class="built_in">super</span>(Conv_Block, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layer=nn.Sequential(</span><br><span class="line">            <span class="comment">#第一个卷积</span></span><br><span class="line">            nn.Conv2d(in_channel,out_channel,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>,padding_mode=<span class="string">&#x27;reflect&#x27;</span>,bias=<span class="literal">False</span>), <span class="comment">#3，1，1是 kernel_size=3 、padding=1 、strike=1</span></span><br><span class="line">            nn.BatchNorm2d(out_channel),    <span class="comment">#将输出归一化</span></span><br><span class="line">            nn.Dropout2d(<span class="number">0.3</span>),          <span class="comment">#随机化</span></span><br><span class="line">            nn.LeakyReLU(),     <span class="comment">#激活函数</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#第二个卷积同理</span></span><br><span class="line">            nn.Conv2d(out_channel, out_channel, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, padding_mode=<span class="string">&#x27;reflect&#x27;</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">            nn.Dropout2d(<span class="number">0.3</span>),</span><br><span class="line">            nn.LeakyReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment">#前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.layer(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#下采样</span></span><br><span class="line"><span class="comment">#收缩路径中的每个步骤都会使用2×2最大池化图层对要素地图进行缩减采样。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DownSample</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,channel</span>):</span><br><span class="line">        <span class="built_in">super</span>(DownSample, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layer=nn.Sequential(</span><br><span class="line">            nn.Conv2d(channel,channel,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,padding_mode=<span class="string">&#x27;reflect&#x27;</span>,bias=<span class="literal">False</span>),  <span class="comment">#3*3 步长为2（缩为1/2） padding为1</span></span><br><span class="line">            nn.BatchNorm2d(channel),</span><br><span class="line">            nn.LeakyReLU()</span><br><span class="line">        )</span><br><span class="line">    <span class="comment">#前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.layer(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#上采样</span></span><br><span class="line"><span class="comment">#扩展路径中的每一步都使用向上2×2卷积对要素地图进行向上采样。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UpSample</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,channel</span>):</span><br><span class="line">        <span class="built_in">super</span>(UpSample, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layer=nn.Conv2d(channel,channel//<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x,feature_map</span>):</span><br><span class="line">        up=F.interpolate(x,scale_factor=<span class="number">2</span>,mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">        out=<span class="variable language_">self</span>.layer(up)</span><br><span class="line">        <span class="keyword">return</span> torch.cat((out,feature_map),dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Unet网络 跟图对应</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(UNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.c1=Conv_Block(<span class="number">3</span>,<span class="number">64</span>)    <span class="comment">#两层卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.d1=DownSample(<span class="number">64</span>)      <span class="comment">#下采样</span></span><br><span class="line">        <span class="variable language_">self</span>.c2=Conv_Block(<span class="number">64</span>,<span class="number">128</span>)  <span class="comment">#两层卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.d2=DownSample(<span class="number">128</span>)     <span class="comment">#下采样</span></span><br><span class="line">        <span class="variable language_">self</span>.c3=Conv_Block(<span class="number">128</span>,<span class="number">256</span>) <span class="comment">#两层卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.d3=DownSample(<span class="number">256</span>)     <span class="comment">#下采样</span></span><br><span class="line">        <span class="variable language_">self</span>.c4=Conv_Block(<span class="number">256</span>,<span class="number">512</span>) <span class="comment">#两层卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.d4=DownSample(<span class="number">512</span>)     <span class="comment">#下采样</span></span><br><span class="line">        <span class="variable language_">self</span>.c5=Conv_Block(<span class="number">512</span>,<span class="number">1024</span>)    <span class="comment">#两层卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.u1=UpSample(<span class="number">1024</span>)      <span class="comment">#上采样</span></span><br><span class="line">        <span class="variable language_">self</span>.c6=Conv_Block(<span class="number">1024</span>,<span class="number">512</span>)    <span class="comment">#两层卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.u2 = UpSample(<span class="number">512</span>)     <span class="comment">#上采样</span></span><br><span class="line">        <span class="variable language_">self</span>.c7 = Conv_Block(<span class="number">512</span>, <span class="number">256</span>)  <span class="comment">#两层卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.u3 = UpSample(<span class="number">256</span>)     <span class="comment">#上采样</span></span><br><span class="line">        <span class="variable language_">self</span>.c8 = Conv_Block(<span class="number">256</span>, <span class="number">128</span>)  <span class="comment">#两层卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.u4 = UpSample(<span class="number">128</span>)     <span class="comment">#上采样</span></span><br><span class="line">        <span class="variable language_">self</span>.c9 = Conv_Block(<span class="number">128</span>, <span class="number">64</span>)   <span class="comment">#两层卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.out = nn.Conv2d(<span class="number">64</span>,num_classes,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)    <span class="comment">#输出</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#步骤</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        R1=<span class="variable language_">self</span>.c1(x)</span><br><span class="line">        R2=<span class="variable language_">self</span>.c2(<span class="variable language_">self</span>.d1(R1))</span><br><span class="line">        R3 = <span class="variable language_">self</span>.c3(<span class="variable language_">self</span>.d2(R2))</span><br><span class="line">        R4 = <span class="variable language_">self</span>.c4(<span class="variable language_">self</span>.d3(R3))</span><br><span class="line">        R5 = <span class="variable language_">self</span>.c5(<span class="variable language_">self</span>.d4(R4))</span><br><span class="line">        O1=<span class="variable language_">self</span>.c6(<span class="variable language_">self</span>.u1(R5,R4))</span><br><span class="line">        O2 = <span class="variable language_">self</span>.c7(<span class="variable language_">self</span>.u2(O1, R3))</span><br><span class="line">        O3 = <span class="variable language_">self</span>.c8(<span class="variable language_">self</span>.u3(O2, R2))</span><br><span class="line">        O4 = <span class="variable language_">self</span>.c9(<span class="variable language_">self</span>.u4(O3, R1))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.out(O4)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x=torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">256</span>,<span class="number">256</span>)</span><br><span class="line">    net=UNet()</span><br><span class="line">    <span class="built_in">print</span>(net(x).shape)</span><br></pre></td></tr></table></figure><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> net <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>) <span class="comment">#看设备</span></span><br><span class="line">weight_path = <span class="string">&#x27;params/unet.pth&#x27;</span>     <span class="comment">#权重地址</span></span><br><span class="line">data_path = <span class="string">r&#x27;data&#x27;</span>     <span class="comment">#地址</span></span><br><span class="line">save_path = <span class="string">&#x27;train_image&#x27;</span>   <span class="comment">#保存地址</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    num_classes = <span class="number">2</span> + <span class="number">1</span>  <span class="comment"># +1是背景也为一类</span></span><br><span class="line">    data_loader = DataLoader(MyDataset(data_path), batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    net = UNet(num_classes).to(device)</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(weight_path):</span><br><span class="line">        net.load_state_dict(torch.load(weight_path))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;successful load weight！&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;not successful load weight&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    opt = optim.Adam(net.parameters())      <span class="comment">#优化器</span></span><br><span class="line">    loss_fun = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    epoch = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> epoch &lt; <span class="number">10</span>:</span><br><span class="line">        <span class="keyword">for</span> i, (image, segment_image) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm.tqdm(data_loader)):</span><br><span class="line">            image, segment_image = image.to(device), segment_image.to(device)</span><br><span class="line">            out_image = net(image)</span><br><span class="line">            train_loss = loss_fun(out_image, segment_image.long())</span><br><span class="line">            opt.zero_grad()</span><br><span class="line"></span><br><span class="line">            train_loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;epoch&#125;</span>-<span class="subst">&#123;i&#125;</span>-train_loss===&gt;&gt;<span class="subst">&#123;train_loss.item()&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            _image = image[<span class="number">0</span>]</span><br><span class="line">            _segment_image = torch.unsqueeze(segment_image[<span class="number">0</span>], <span class="number">0</span>) * <span class="number">255</span></span><br><span class="line">            _out_image = torch.argmax(out_image[<span class="number">0</span>], dim=<span class="number">0</span>).unsqueeze(<span class="number">0</span>) * <span class="number">255</span></span><br><span class="line"></span><br><span class="line">            img = torch.stack([_segment_image, _out_image], dim=<span class="number">0</span>)</span><br><span class="line">            save_image(img, <span class="string">f&#x27;<span class="subst">&#123;save_path&#125;</span>/<span class="subst">&#123;i&#125;</span>.png&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            torch.save(net.state_dict(), weight_path)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;save successfully!&#x27;</span>)</span><br><span class="line">        epoch += <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Unet手写代码笔记&quot;&gt;&lt;a href=&quot;#Unet手写代码笔记&quot; class=&quot;headerlink&quot; title=&quot;Unet手写代码笔记&quot;&gt;&lt;/a&gt;Unet手写代码笔记&lt;/h1&gt;&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerli</summary>
      
    
    
    
    
    <category term="代码复现" scheme="https://1kunn.github.io/tags/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>怎么找创新点</title>
    <link href="https://1kunn.github.io/2024/09/03/%E6%80%8E%E4%B9%88%E6%89%BE%E5%88%9B%E6%96%B0%E7%82%B9/"/>
    <id>https://1kunn.github.io/2024/09/03/%E6%80%8E%E4%B9%88%E6%89%BE%E5%88%9B%E6%96%B0%E7%82%B9/</id>
    <published>2024-09-03T03:10:00.000Z</published>
    <updated>2024-09-04T01:09:40.855Z</updated>
    
    <content type="html"><![CDATA[<h1 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h1><h2 id="什么是创新点"><a href="#什么是创新点" class="headerlink" title="什么是创新点"></a>什么是创新点</h2><p>创新靠编，不是指从无到有，而是胶水，或者说缝合。</p><p><strong>缝合</strong>：</p><p>牛肉：好吃但是贵</p><p>西兰花：一般但是便宜</p><p>所以 牛肉+西兰花 ：好吃又便宜</p><h2 id="怎么找创新点"><a href="#怎么找创新点" class="headerlink" title="怎么找创新点"></a>怎么找创新点</h2><ul><li><p>假设你找到一个基准模型<strong>transform</strong> 。</p></li><li><p>去GPT问它知道这个模型的缺点吗。</p></li><li><p>再去翻文献，找能解决某个缺点的模块缝进去，跑通即可。</p></li></ul><p><img src="F:\Typora图\image-20240904074838446-17254073200771.png" alt="image-20240904074838446"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;创新点&quot;&gt;&lt;a href=&quot;#创新点&quot; class=&quot;headerlink&quot; title=&quot;创新点&quot;&gt;&lt;/a&gt;创新点&lt;/h1&gt;&lt;h2 id=&quot;什么是创新点&quot;&gt;&lt;a href=&quot;#什么是创新点&quot; class=&quot;headerlink&quot; title=&quot;什么是创新点&quot;&gt;&lt;</summary>
      
    
    
    
    
    <category term="理论" scheme="https://1kunn.github.io/tags/%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>快速高效查找论文、代码及数据集，论文对应项目</title>
    <link href="https://1kunn.github.io/2024/09/02/%E5%BF%AB%E9%80%9F%E9%AB%98%E6%95%88%E6%9F%A5%E6%89%BE%E8%AE%BA%E6%96%87%E3%80%81%E4%BB%A3%E7%A0%81%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E8%AE%BA%E6%96%87%E5%AF%B9%E5%BA%94%E9%A1%B9%E7%9B%AE/"/>
    <id>https://1kunn.github.io/2024/09/02/%E5%BF%AB%E9%80%9F%E9%AB%98%E6%95%88%E6%9F%A5%E6%89%BE%E8%AE%BA%E6%96%87%E3%80%81%E4%BB%A3%E7%A0%81%E5%8F%8A%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E8%AE%BA%E6%96%87%E5%AF%B9%E5%BA%94%E9%A1%B9%E7%9B%AE/</id>
    <published>2024-09-02T13:35:39.000Z</published>
    <updated>2024-09-03T02:10:21.408Z</updated>
    
    <content type="html"><![CDATA[<h1 id="科研技能"><a href="#科研技能" class="headerlink" title="科研技能"></a>科研技能</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>通过这些可以轻松找到某个研究领域对应的论文和代码、数据集、提升自身代码能力。</p><p>1.<a href="https://paperswithcode.com/(%E8%AE%BA%E6%96%87%E5%AF%B9%E5%BA%94%E4%BB%A3%E7%A0%81)">https://paperswithcode.com/(论文对应代码)</a> </p><p>2.<a href="https://scholar.google.com/(%E8%B0%B7%E6%AD%8C%E5%AD%A6%E6%9C%AF)">https://scholar.google.com/(谷歌学术)</a> </p><p>3.<a href="https://datasetsearch.research.google.com/(%E8%B0%B7%E6%AD%8C%E6%95%B0%E6%8D%AE%E9%9B%86)">https://datasetsearch.research.google.com/(谷歌数据集)</a> </p><p>4.<a href="https://www.kaggle.com/datasets(kaggle%E6%95%B0%E6%8D%AE%E9%9B%86)">https://www.kaggle.com/datasets(kaggle数据集)</a> </p><p>5.<a href="https://nn.labml.ai/index.html(%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%90%84%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%B8%A6%E6%B3%A8%E9%87%8A)">https://nn.labml.ai/index.html(深度学习各网络代码带注释)</a></p><h2 id="找论文、代码"><a href="#找论文、代码" class="headerlink" title="找论文、代码"></a>找论文、代码</h2><p>网站：</p><p>按老师要求找对应的论文技术</p><p><img src="F:\Typora图\image-20240903080908772-17253221506091.png" alt="image-20240903080908772"></p><p>本次进入语义分割</p><p><img src="F:\Typora图\image-20240903080957850-17253222005953.png" alt="image-20240903080957850"></p><p>找到一篇文献</p><p><img src="F:\Typora图\image-20240903081330741-17253224137665.png" alt="image-20240903081330741"></p><h2 id="项目复现并构建自己项目baseline"><a href="#项目复现并构建自己项目baseline" class="headerlink" title="项目复现并构建自己项目baseline"></a>项目复现并构建自己项目baseline</h2><p>本次选用上图中的milesial&#x2F;Pytorch-UNet</p><p>项目地址：<a href="https://github.com/milesial/Pytorch-UNet">https://github.com/milesial/Pytorch-UNet</a> </p><ul><li><strong>先下载代码</strong></li></ul><p><img src="F:\Typora图\image-20240903081722672.png" alt="image-20240903081722672"></p><ul><li><p><strong>在Anaconda Prompt中构建环境</strong></p><p>conda create -n unet4 python&#x3D;3.9</p><p><img src="F:\Typora图\image-20240903083615216.png" alt="image-20240903083615216"></p></li><li><p><strong>激活环境</strong></p><p>conda activate unet4</p><p><img src="F:\Typora图\image-20240903083652911.png" alt="image-20240903083652911"></p></li><li><p><strong>查看cuda版本</strong>（使用N卡才有 没有显卡的就用cpu版本的PyTorch）</p><p>nvidia-smi</p><p><img src="F:\Typora图\image-20240903083917714.png" alt="image-20240903083917714"></p></li><li><p><strong>找合适版本的Pytorch</strong></p><p>网站：<a href="https://pytorch.org/">https://pytorch.org/</a></p><p><img src="F:\Typora图\image-20240903084237034-17253241590688.png" alt="image-20240903084237034"></p><p>本次选用这个版本</p><p><img src="F:\Typora图\image-20240903084350066-172532423172010.png" alt="image-20240903084350066"></p></li><li><p><strong>下载PyTorch</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia</span><br></pre></td></tr></table></figure></li></ul><p>在对应环境下下载</p><p><img src="F:\Typora图\image-20240903084452049.png" alt="image-20240903084452049"></p><ul><li><p>在pycharm中打开代码</p><p><img src="F:\Typora图\image-20240903084557014.png" alt="image-20240903084557014"></p><p>按README继续</p></li></ul><h2 id="查找数据集"><a href="#查找数据集" class="headerlink" title="查找数据集"></a>查找数据集</h2><p>网站：<a href="https://datasetsearch.research.google.com/">https://datasetsearch.research.google.com/</a></p><p>网站：<a href="https://www.kaggle.com/datasets(kaggle%E6%95%B0%E6%8D%AE%E9%9B%86)">https://www.kaggle.com/datasets(kaggle数据集)</a> </p><p>进去按找适合的数据集即可（语义分割、分类任务啥的）</p><h2 id="提升代码能力"><a href="#提升代码能力" class="headerlink" title="提升代码能力"></a>提升代码能力</h2><p><a href="https://nn.labml.ai/index.html(%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%90%84%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%B8%A6%E6%B3%A8%E9%87%8A)">https://nn.labml.ai/index.html(深度学习各网络代码带注释)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;科研技能&quot;&gt;&lt;a href=&quot;#科研技能&quot; class=&quot;headerlink&quot; title=&quot;科研技能&quot;&gt;&lt;/a&gt;科研技能&lt;/h1&gt;&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h</summary>
      
    
    
    
    
    <category term="理论" scheme="https://1kunn.github.io/tags/%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch深度学习框架</title>
    <link href="https://1kunn.github.io/2024/09/02/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    <id>https://1kunn.github.io/2024/09/02/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/</id>
    <published>2024-09-02T02:07:35.000Z</published>
    <updated>2024-09-02T12:47:55.783Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch深度学习框架"><a href="#PyTorch深度学习框架" class="headerlink" title="PyTorch深度学习框架"></a>PyTorch深度学习框架</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>理论结合代码详解MNIST手写数据集图像分类，逐行代码进行讲解，训练后并绘制loss图以及acc图，训练完成保存权重文件，将权重文件加载进模型进行推理，输入十张图像推理出它们所属类别。</p><p><strong>MNIST</strong>：手写数字公共数据集，具体是0-9；图片颜色用像素值区分 0-255， 0是白，255是黑；一张图有28*28&#x3D;784像素点</p><p><strong>输入层</strong>： 将原始数据通过特征提取，向量化特征成为输入层。</p><p><strong>隐含层</strong>： 将提出的数据进行权重计算。</p><p><strong>输出层</strong>： 输出结果（这里是分类数据0-9，所以有10个输出）。</p><p><strong>权重和偏置</strong>：输入层到隐含层的计算。</p><p><strong>反向传播</strong>： 根据损失函数计算出的误差，通过链式法则（Chain Rule）逐层计算并更新网络中的参数（权重和偏置）以最小化误差的过程。具体步骤如下：</p><p>​1.<u>计算损失</u>：使用损失函数计算预测值与真实值之间的误差。例如，使用均方误差（MSE）或交叉熵损失。<br>​2.<u>误差反向传播</u>：从输出层开始，计算损失相对于每个参数的梯度。通过链式法则，将梯度逐层传递回去。<br>​3.<u>参数更新</u>：使用优化算法（如梯度下降）更新每个参数，使损失最小化。</p><p><img src="F:\Typora图\image-20240902103608810-17252445703315.png" alt="image-20240902103608810"></p><h2 id="神经网络架构解析"><a href="#神经网络架构解析" class="headerlink" title="神经网络架构解析"></a>神经网络架构解析</h2><ol><li><p>input接收到图像后先在<strong>输入样本</strong> 中按像素点展开，因为全连接层（<strong>隐含层</strong>）只能接收一维的数据。</p><p><img src="F:\Typora图\image-20240902103703012-17252446243757.png" alt="image-20240902103703012"></p></li><li><p>输入层到隐含层经过了运算，按权重配比如下所示</p><p>W是权重、b是偏置、X是输入数据<br>$$<br>X11<em>W11+X12</em>W12+…+X1i*W1i+b&#x3D;隐含层1<br>$$</p><p>$$<br>X11<em>W21+X12</em>W22+…+X1i*W2i+b&#x3D;隐含层2<br>$$</p><p> …以此类推<img src="F:\Typora图\image-20240902103541462-17252445431863.png" alt="image-20240902103541462"></p></li><li><p>计算后得出结果分类为0-9.</p><p>softmax会把值改为概率值，比如该图为0的概率是0.05，为1的概率是0.04…为3的概率是0.8.所以就输出该图识别为3.</p></li></ol><h2 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h2><p>按图例的内容实现</p><p>环境： python3.9  、PyTorch&#x3D;conda install pytorch&#x3D;&#x3D;2.0.0 torchvision&#x3D;&#x3D;0.15.0 torchaudio&#x3D;&#x3D;2.0.0 pytorch-   cuda&#x3D;11.8 -c pytorch -c nvidia</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim <span class="comment">#导入优化器</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms  <span class="comment">#torchvision是计算机视觉的一个库、 datasets数据集</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader <span class="comment">#数据加载的 把图片打包好给输入层</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#绘图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">21</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查cuda是否有可用的GPU</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using device: <span class="subst">&#123;device&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义数据预处理</span></span><br><span class="line">transform = transforms.Compose([     <span class="comment">#Compose是一个类，用于将多个图像预处理操作组合到一起</span></span><br><span class="line">    transforms.ToTensor(), <span class="comment">#将图像转化为Totensor张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,)) <span class="comment">#标准化图像数据 Normalize将不同尺度、分布的数据转化为统一形式。 因为是灰度图所以就一个0.5 如果是彩色图就要三个0.5</span></span><br><span class="line">])                                          <span class="comment">#所谓的通道就是几个0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#加载MNIST数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform) <span class="comment">#下载训练集</span></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)  <span class="comment">#下载测试集 下载到二级文件夹data中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建数据加载器</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>) <span class="comment">#对数据集打包，指定批次为64（一批次训练64张数据）</span></span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>) <span class="comment">#shuffle随机打乱数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#改进的三层神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">o1Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#下面定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">256</span>) <span class="comment">#第一层 输入28*28图像，输出256个神经元（展平）</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">256</span>, <span class="number">128</span>) <span class="comment">#第二层，输入256 输出128 （隐藏层）</span></span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">128</span>, <span class="number">64</span>) <span class="comment">#第三层 输入128 输出64 （隐藏层）</span></span><br><span class="line">        <span class="variable language_">self</span>.fc4 = nn.Linear(<span class="number">64</span>, <span class="number">10</span>) <span class="comment">#第四层 输入64神经元 输出10个类别 （隐藏层）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#具体操作</span></span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>) <span class="comment">#展开数据，维数为1</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x)) <span class="comment">#第一层 ReLu激活</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc2(x)) <span class="comment">#第二层 ReLu激活</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc3(x)) <span class="comment">#第三层 ReLu激活</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc4(x) <span class="comment">#第四层（输出层不需要激活函数） nn.CrossEntropyLoss()会在内部进行softmax操作</span></span><br><span class="line">        <span class="comment"># x = torch.softmax(x, dim=1) #使用softmax激活函数</span></span><br><span class="line">        <span class="keyword">return</span> x <span class="comment">#返回概率值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型，并将模型移动到GPU上</span></span><br><span class="line">model = o1Net().to(device) <span class="comment">#实例化类</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment">#交叉熵</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>) <span class="comment">#使用Adam优化器，学习率设置为0.001</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用于保存训练过程中的损失和准确率</span></span><br><span class="line">train_losses = []</span><br><span class="line">train_accuracies = []</span><br><span class="line">test_accuracies = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">epochs = <span class="number">100</span> <span class="comment">#训练次数</span></span><br><span class="line">best_accuracy = <span class="number">0.0</span> <span class="comment">#记录最佳验证集准确率</span></span><br><span class="line">best_model_path = <span class="string">&#x27;best_mnist_model.pth&#x27;</span>  <span class="comment">#保存最佳模型路径 最佳权重文件路径</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs): <span class="comment">#训练 每次开始时清空原先数据</span></span><br><span class="line">    running_loss = <span class="number">0.0</span> <span class="comment">#损失</span></span><br><span class="line">    correct_train = <span class="number">0</span> <span class="comment">#正确样本数量</span></span><br><span class="line">    total_train = <span class="number">0</span> <span class="comment">#样本总数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#训练过程</span></span><br><span class="line">    model.train() <span class="comment">#设定模型为训练模式</span></span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader: <span class="comment">#从训练集中读取图像和相对应的labels（图像为3 label=3）</span></span><br><span class="line">        inputs, labels = inputs.to(device), labels.to(device) <span class="comment">#将数据移动到GPU上</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#梯度清零</span></span><br><span class="line">        outputs = model(inputs) <span class="comment">#前向传播</span></span><br><span class="line">        loss = criterion(outputs, labels) <span class="comment">#输出的结果值与label进行计算 得到损失值</span></span><br><span class="line">        loss.backward() <span class="comment">#反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#更新参数</span></span><br><span class="line">        running_loss = running_loss+loss.item() <span class="comment">#累加损失</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#计算训练集上的准确率</span></span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>) <span class="comment">#获取预测结果  _取最大值 predicted取最大值索引</span></span><br><span class="line">        total_train += labels.size(<span class="number">0</span>) <span class="comment">#累加样本数量</span></span><br><span class="line">        correct_train += (predicted == labels).<span class="built_in">sum</span>().item()  <span class="comment">#累加正确预测的数量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算测试集上的准确率</span></span><br><span class="line">    train_accuracy = correct_train / total_train</span><br><span class="line">    train_accuracies.append(running_loss / <span class="built_in">len</span>(train_loader))  <span class="comment"># 记录每个 epoch 的测试集准确率 累加的loss/数量</span></span><br><span class="line">    train_accuracies.append(train_accuracy)  <span class="comment"># 记录每个 epoch 的训练集准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>, Loss: <span class="subst">&#123;running_loss/<span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>, Train Accuracy: <span class="subst">&#123;train_accuracy:<span class="number">.2</span>%&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在测试集上评估模型</span></span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 设定模型为评估模式</span></span><br><span class="line">    correct = <span class="number">0</span>  <span class="comment"># 正确的预测数量</span></span><br><span class="line">    total = <span class="number">0</span>  <span class="comment"># 样本总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 关闭梯度计算</span></span><br><span class="line">        <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">            inputs, labels = inputs.to(device), labels.to(device)  <span class="comment"># 将数据移动到GPU上</span></span><br><span class="line">            outputs = model(inputs)  <span class="comment"># 前向传播</span></span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)  <span class="comment"># 获取预测结果</span></span><br><span class="line">            total += labels.size(<span class="number">0</span>)  <span class="comment"># 累加样本数量</span></span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()  <span class="comment"># 累加正确预测的数量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算测试集上的准确率</span></span><br><span class="line">    test_accuracy = correct / total</span><br><span class="line">    test_accuracies.append(test_accuracy)  <span class="comment"># 记录每个 epoch 的测试集准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>, Test Accuracy: <span class="subst">&#123;test_accuracy:<span class="number">.2</span>%&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果测试集准确率提高，保存当前模型的权重</span></span><br><span class="line">    <span class="keyword">if</span> test_accuracy &gt; best_accuracy:</span><br><span class="line">        best_accuracy = test_accuracy</span><br><span class="line">        torch.save(model.state_dict(), best_model_path) <span class="comment">#保存模型参数</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Best model saved with accuracy: <span class="subst">&#123;best_accuracy:<span class="number">.2</span>%&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best Accuracy on test set: <span class="subst">&#123;best_accuracy:<span class="number">.2</span>%&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制并保存损失和准确率曲线</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制损失曲线</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 选择第一个子图</span></span><br><span class="line">plt.plot(train_losses, label=<span class="string">&#x27;Training Loss&#x27;</span>)  <span class="comment"># 传入数据、设置标签为Training Loss</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)  <span class="comment"># x轴数据</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss over Epochs&#x27;</span>)  <span class="comment"># 设置标签</span></span><br><span class="line">plt.legend()  <span class="comment"># 添加图例</span></span><br><span class="line">plt.grid(<span class="literal">True</span>)  <span class="comment"># 添加网格</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制训练集和测试集准确率曲线</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(train_accuracies, label=<span class="string">&#x27;Train Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(test_accuracies, label=<span class="string">&#x27;Test Accuracy&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Train and Test Accuracy over Epochs&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存图像</span></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">&#x27;loss_and_accuracy_curves.png&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;PyTorch深度学习框架&quot;&gt;&lt;a href=&quot;#PyTorch深度学习框架&quot; class=&quot;headerlink&quot; title=&quot;PyTorch深度学习框架&quot;&gt;&lt;/a&gt;PyTorch深度学习框架&lt;/h1&gt;&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; cla</summary>
      
    
    
    
    
    <category term="理论" scheme="https://1kunn.github.io/tags/%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>复现-unet2</title>
    <link href="https://1kunn.github.io/2024/09/01/%E5%A4%8D%E7%8E%B0-unet2/"/>
    <id>https://1kunn.github.io/2024/09/01/%E5%A4%8D%E7%8E%B0-unet2/</id>
    <published>2024-09-01T06:48:44.000Z</published>
    <updated>2024-09-02T01:13:33.883Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Github代码复现2"><a href="#Github代码复现2" class="headerlink" title="Github代码复现2"></a>Github代码复现2</h1><h2 id="前置内容"><a href="#前置内容" class="headerlink" title="前置内容"></a>前置内容</h2><p>视频教程：<a href="https://www.bilibili.com/video/BV1uW421d72j/?spm_id_from=333.880.my_history.page.click&vd_source=49378ece86ab83ee76bcaac1cc96562c">https://www.bilibili.com/video/BV1uW421d72j/?spm_id_from=333.880.my_history.page.click&amp;vd_source=49378ece86ab83ee76bcaac1cc96562c</a></p><p>Github地址：<a href="https://github.com/4uiiurz1/pytorch-nested-unet">https://github.com/4uiiurz1/pytorch-nested-unet</a></p><p>数据集地址：<a href="https://www.kaggle.com/c/data-science-bowl-2018/data">https://www.kaggle.com/c/data-science-bowl-2018/data</a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>代码因为是2018年的，所以有些包不适合版本，甚至是作者给出的版本也会出bug。</p><p>本次使用stage1_train数据集</p><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>python&#x3D;3.9</p><p>PyTorch&#x3D;conda install pytorch&#x3D;&#x3D;2.0.0 torchvision&#x3D;&#x3D;0.15.0 torchaudio&#x3D;&#x3D;2.0.0 pytorch-cuda&#x3D;11.8 -c pytorch -c nvidia</p><p>pip install -r requirements.txt</p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><ol><li>下载代码</li></ol><p><img src="F:\Typora图\image-20240901210613867-17251959760353.png" alt="image-20240901210613867"></p><ol start="2"><li><p>打开<img src="F:\Typora图\image-20240901210811038.png" alt="image-20240901210811038"></p></li><li><p>环境搭建如上</p></li><li><p>数据集下载</p></li></ol><p><img src="F:\Typora图\image-20240901205852176-17251955341241.png" alt="image-20240901205852176"></p><p>数据集要按readme要求放置<img src="F:\Typora图\image-20240901211045505-17251962484367.png" alt="image-20240901211045505"></p><p><img src="F:\Typora图\image-20240901211123370-17251962869379.png" alt="image-20240901211123370"></p><ol start="5"><li><p>预处理数据</p><p>python preprocess_dsb2018.py</p><p><img src="F:\Typora图\image-20240901211008360-17251962121195.png" alt="image-20240901211008360"></p></li><li><p>预训练模型</p><p>正常来说会有这个报错<img src="F:\Typora图\image-20240902074032023-17252340338301.png" alt="image-20240902074032023"></p><p>把albumentations降到1.2.0版本即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install albumentations==<span class="number">1.2</span><span class="number">.0</span></span><br></pre></td></tr></table></figure><p>python train.py –dataset dsb2018_96 –arch NestedUNet</p><p><img src="F:\Typora图\image-20240901211214406-172519633841011.png" alt="image-20240901211214406"></p></li><li><p>结果</p><p>运行结果会出现在models文件目录下</p><p><img src="F:\Typora图\image-20240901211333991-172519641557113.png" alt="image-20240901211333991"></p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Github代码复现2&quot;&gt;&lt;a href=&quot;#Github代码复现2&quot; class=&quot;headerlink&quot; title=&quot;Github代码复现2&quot;&gt;&lt;/a&gt;Github代码复现2&lt;/h1&gt;&lt;h2 id=&quot;前置内容&quot;&gt;&lt;a href=&quot;#前置内容&quot; class=&quot;</summary>
      
    
    
    
    
    <category term="代码复现" scheme="https://1kunn.github.io/tags/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>复现_unet1</title>
    <link href="https://1kunn.github.io/2024/08/23/%E5%A4%8D%E7%8E%B0-unet1/"/>
    <id>https://1kunn.github.io/2024/08/23/%E5%A4%8D%E7%8E%B0-unet1/</id>
    <published>2024-08-23T02:16:25.000Z</published>
    <updated>2024-08-23T02:55:23.021Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Github代码复现1"><a href="#Github代码复现1" class="headerlink" title="Github代码复现1"></a>Github代码复现1</h1><h2 id="前置内容"><a href="#前置内容" class="headerlink" title="前置内容"></a>前置内容</h2><ul><li><p>代码地址：<a href="https://github.com/Andy-zhujunwen/UNET-ZOO">https://github.com/Andy-zhujunwen/UNET-ZOO</a></p></li><li><p>视频演示地址：<a href="https://www.bilibili.com/video/BV1Um421V7jW/?spm_id_from=333.880.my_history.page.click&vd_source=49378ece86ab83ee76bcaac1cc96562c">https://www.bilibili.com/video/BV1Um421V7jW/?spm_id_from=333.880.my_history.page.click&amp;vd_source=49378ece86ab83ee76bcaac1cc96562c</a></p></li><li><p>数据集：</p><p>the Cell dataset(dsb2018) link：<a href="https://pan.baidu.com/s/1BaVrzYdrSP78CwYaRzZr1w">https://pan.baidu.com/s/1BaVrzYdrSP78CwYaRzZr1w</a> keyword：5l54</p><p>the Liver dataset: link：<a href="https://pan.baidu.com/s/1FljGCVzu7HPYpwAKvSVN4Q">https://pan.baidu.com/s/1FljGCVzu7HPYpwAKvSVN4Q</a> keyword：5l88</p><p>the Cell dataset(isbi) link：<a href="https://pan.baidu.com/s/1FkfnhU-RnYFZti62-f8AVA">https://pan.baidu.com/s/1FkfnhU-RnYFZti62-f8AVA</a> keyword：14rz</p><p>the Lung dataset: link：<a href="https://pan.baidu.com/s/1sLFRmtG2TOTEgUKniJf7AA">https://pan.baidu.com/s/1sLFRmtG2TOTEgUKniJf7AA</a> keyword：qdwo</p><p>the Corneal Nerve dataset: link：<a href="https://pan.baidu.com/s/1T3-kS_FgYI6DeXv3n1I7bA">https://pan.baidu.com/s/1T3-kS_FgYI6DeXv3n1I7bA</a> keyword：ih02</p><p>the Eye Vessels(DRIVE dataset) link：<a href="https://pan.baidu.com/s/1UkMLmdbM61N8ecgnKlAsPg">https://pan.baidu.com/s/1UkMLmdbM61N8ecgnKlAsPg</a> keyword：f1ek</p><p>the Esophagus and Esophagus Cancer dataset from First Affiliated Hospital of Sun Yat-sen University link：<a href="https://pan.baidu.com/s/10b5arIQjNpiggwdkgYNHXQ">https://pan.baidu.com/s/10b5arIQjNpiggwdkgYNHXQ</a> keyword：hivm</p></li></ul><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>这次复现算是比较简单的一种类型。</p><p>就用了 Liver 数据集</p><p>运行用这个（在pycharm终端运行）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.py --action train --arch UNet --epoch <span class="number">21</span> --batch_size <span class="number">6</span> --dataset liver</span><br></pre></td></tr></table></figure><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>在Anaconda Navigator中建立环境unet2</p><p>环境下安装python3.6+pytorch1.3.1</p><p>后面还会报错 缺啥就安啥（sklearn库、<code>skimage</code>库、OpenCV……）</p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><ol><li>Github页面中Code里下载压缩包并解压到本地</li></ol><p> <img src="F:\Typora图\image-20240823103139173-17243803037891-17243803091453.png" alt="image-20240823103139173"></p><ol start="2"><li><p>环境搭建如上</p></li><li><p>pycharm中打开<img src="F:\Typora图\image-20240823103827404-17243807102195.png" alt="image-20240823103827404"></p></li><li><p>下载数据集并解压到文件夹中</p><p><img src="F:\Typora图\image-20240823104022502-17243808241787.png" alt="image-20240823104022502"></p></li><li><p>按文档说明修改dataset（因为只用liver数据集，所以就修改这里即可）</p><p><img src="F:\Typora图\image-20240823104728770-17243812503489.png" alt="image-20240823104728770"></p></li><li><p>运行（要等好久）最后在result里会有结果图</p><p><img src="F:\Typora图\image-20240823104941521.png" alt="image-20240823104941521"></p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Github代码复现1&quot;&gt;&lt;a href=&quot;#Github代码复现1&quot; class=&quot;headerlink&quot; title=&quot;Github代码复现1&quot;&gt;&lt;/a&gt;Github代码复现1&lt;/h1&gt;&lt;h2 id=&quot;前置内容&quot;&gt;&lt;a href=&quot;#前置内容&quot; class=&quot;</summary>
      
    
    
    
    
    <category term="代码复现" scheme="https://1kunn.github.io/tags/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>最重要的事</title>
    <link href="https://1kunn.github.io/2024/08/20/%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%8B/"/>
    <id>https://1kunn.github.io/2024/08/20/%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%8B/</id>
    <published>2024-08-20T01:20:43.000Z</published>
    <updated>2024-08-21T07:00:50.661Z</updated>
    
    <content type="html"><![CDATA[<h1 id="人生最重要的事"><a href="#人生最重要的事" class="headerlink" title="人生最重要的事"></a>人生最重要的事</h1><p>没有一直稳定的行业，所以最重要的是婚姻，就是那个与你相伴一生的人。</p><p>良人未必一开始就能遇到良人。但非良人，无一例外的归宿都是动物性。</p><p><strong>一个不爱自己的人，无法及时察觉他人的自私；而一个只爱自己的人，无法想象ta人的无私。唯有自知，方有自爱；唯有自爱，方有人爱。</strong></p><p><strong>去珍惜那个把你摆到第一位的人，愿意为你付出的人，这样就能看到爱意的流动。（反人性的）</strong></p><p>婚姻是<strong>两个人</strong>携手对抗这个世界</p><h2 id="摆正态度"><a href="#摆正态度" class="headerlink" title="摆正态度"></a>摆正态度</h2><p>人有很多社会关系。</p><p>职场上的雇佣关系</p><p>朋友间的社交关系</p><p>唯独婚姻是战友关系！！！</p><p><strong><u>所以我要找的是一个相互爱慕的战友 同目标共进退 不抛弃不放弃 必要的时候枪口一致对外 其次才是对方的技能外貌等</u></strong></p><h2 id="爱人前先爱自己"><a href="#爱人前先爱自己" class="headerlink" title="爱人前先爱自己"></a>爱人前先爱自己</h2><p><strong>在亲密关系中 ‘你越学如何让别人感觉舒服，只会让你对”如何逃避问题“愈发熟练’</strong></p><p>要求：</p><ul><li><p>正确认识自己，了解自己的优缺点，了解自己喜欢怎样的人，想过怎样的生活，底线在哪里。</p></li><li><p>及时察觉对方的态度，知道感情中已经变质的部分并做出反应。</p></li><li><p>一旦碰到底线且无合理解释立马结束。（边界感）</p></li><li><p>在亲密关系中 <strong>严以律己，同以待人。</strong> 不经考察的 ‘宽以待人’ 是对双方的不负责</p></li><li><p>不要以“财”诱人，保持外在整洁。</p></li><li><p>学会发现对的人，不可强求，更重要的是提高自身的识别能力。</p></li></ul><h2 id="人性"><a href="#人性" class="headerlink" title="人性"></a>人性</h2><p>在另一半遭遇不测和打击时，多年的付出换来的不是一起面对，而是冷静的切割和划清界限</p><p>“精致”背后的分别心与虚荣。</p><p><strong>对生的渴望和对死的恐惧</strong></p><p>我和我身边的人做事有底线，所以我下意识的以为大部分人做事也有底线，不觉得有底线是件多么了不得的事情。</p><h2 id="重新认识女性"><a href="#重新认识女性" class="headerlink" title="重新认识女性"></a>重新认识女性</h2><p>人类基因写死两件事：</p><ul><li>让自己生存</li><li>让自己的基因繁衍</li></ul><p><strong>女性先天有抚育后代的冲动，所以女性主要以优化后代存活率展开，而不是围绕“爱情”展开。</strong></p><p>女性会比男性更“现实”，而现在 现实被认为是一种“成熟”</p><p><strong>不管男女：</strong></p><p><strong>独立、自私、利己、幕强、无责任感和消费主义都是独立的， 不要混为一谈把它们都当作  <em><u>“独立的体现”</u></em></strong></p><p><strong>独立意味着从枷锁中挣脱，需要肩负压力、责任并且得有面对困难的勇气</strong></p><h2 id="体面和祛魅"><a href="#体面和祛魅" class="headerlink" title="体面和祛魅"></a>体面和祛魅</h2><p>精致光鲜的背后都是无法忍受的虚荣、肤浅和分别心（精致利己主义）。</p><p>ta们只在乎外人的评价，只去做那些看起来“体面”的事情。</p><p>真正重要的东西都是很“朴实”的，是要祛魅后用心去看的。（只看人品）</p><p>何为人品：</p><p><strong>责任心永远是1，其余的优点都是后面的0。</strong></p><p><img src="F:\Typora图\image-20240820110319147-17241230042901.png" alt="image-20240820110319147"></p><h2 id="筛选"><a href="#筛选" class="headerlink" title="筛选"></a>筛选</h2><p><strong>首先 如果你喜欢某种品质 不要寄托于对方 而是最好让自己去拥有它</strong></p><p>好的筛选点：男女比例平衡、利益关系不大、具有理想主义色彩、具有定力及智力门槛</p><p>参加线下兴趣小组（健身、读书）</p><h2 id="初步接触"><a href="#初步接触" class="headerlink" title="初步接触"></a>初步接触</h2><p>看整体风格和表情神态</p><p>演习7天</p><p><img src="F:\Typora图\image-20240820120639264-17241268016503.png" alt="image-20240820120639264"></p><h2 id="遇到好关系的信号"><a href="#遇到好关系的信号" class="headerlink" title="遇到好关系的信号"></a>遇到好关系的信号</h2><p>开始热爱自己的生命</p><h2 id="上岸第一剑-恋爱脑的理解"><a href="#上岸第一剑-恋爱脑的理解" class="headerlink" title="上岸第一剑 恋爱脑的理解"></a>上岸第一剑 恋爱脑的理解</h2><p><strong>小时候不懂为什么悟空不直接将唐僧送到到西天。现在才知道，真经不在雷音寺，而是一步步用脚丈量出来的。</strong></p><p><strong>你是什么样的人，就会吸引到什么样的人。 人生垂暮之年，回忆你我往事，你是否能引以为豪？</strong></p><p>上岸第一剑：你不用对我好，你要比我强；你比我进步快的时候可以随时踢掉我。（利己主义）</p><p>恋爱脑： 在囚徒困境中永远选择合作的那个囚徒。（不遵循客观规律）</p><p><strong><u>要求：</u></strong> </p><ul><li>自己首先有能做“合作者的勇气”，以及能看到这些“合作者”身上宝贵的东西。 </li><li>第一张牌永远是合作，后续怎么出要看对方怎么出。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;人生最重要的事&quot;&gt;&lt;a href=&quot;#人生最重要的事&quot; class=&quot;headerlink&quot; title=&quot;人生最重要的事&quot;&gt;&lt;/a&gt;人生最重要的事&lt;/h1&gt;&lt;p&gt;没有一直稳定的行业，所以最重要的是婚姻，就是那个与你相伴一生的人。&lt;/p&gt;
&lt;p&gt;良人未必一开始就能</summary>
      
    
    
    
    
    <category term="最重要的人生大事" scheme="https://1kunn.github.io/tags/%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%BA%E7%94%9F%E5%A4%A7%E4%BA%8B/"/>
    
  </entry>
  
  <entry>
    <title>学术</title>
    <link href="https://1kunn.github.io/2024/08/12/%E5%AD%A6%E6%9C%AF/"/>
    <id>https://1kunn.github.io/2024/08/12/%E5%AD%A6%E6%9C%AF/</id>
    <published>2024-08-12T00:42:25.000Z</published>
    <updated>2024-08-12T02:49:47.486Z</updated>
    
    <content type="html"><![CDATA[<h1 id="定基准模型"><a href="#定基准模型" class="headerlink" title="定基准模型"></a>定基准模型</h1><h2 id="什么是基准模型"><a href="#什么是基准模型" class="headerlink" title="什么是基准模型"></a>什么是基准模型</h2><p>首先找到目标论文</p><p>假设有论文1 &#x3D; A + b + c</p><p>我们就可以选论文基准模型： A 或者直接 论文1整个当基准模型</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;定基准模型&quot;&gt;&lt;a href=&quot;#定基准模型&quot; class=&quot;headerlink&quot; title=&quot;定基准模型&quot;&gt;&lt;/a&gt;定基准模型&lt;/h1&gt;&lt;h2 id=&quot;什么是基准模型&quot;&gt;&lt;a href=&quot;#什么是基准模型&quot; class=&quot;headerlink&quot; title=</summary>
      
    
    
    
    
    <category term="学习记录" scheme="https://1kunn.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>顶刊翻译</title>
    <link href="https://1kunn.github.io/2024/07/23/%E9%A1%B6%E5%88%8A%E7%BF%BB%E8%AF%91/"/>
    <id>https://1kunn.github.io/2024/07/23/%E9%A1%B6%E5%88%8A%E7%BF%BB%E8%AF%91/</id>
    <published>2024-07-22T23:09:57.000Z</published>
    <updated>2024-07-24T09:44:18.417Z</updated>
    
    <content type="html"><![CDATA[<h1 id="需求如下"><a href="#需求如下" class="headerlink" title="需求如下"></a>需求如下</h1><img src="C:\Users\huangruiwen\AppData\Roaming\Typora\typora-user-images\image-20240723071053125.png" style="zoom:80%;" /><h2 id="顶刊选择"><a href="#顶刊选择" class="headerlink" title="顶刊选择"></a>顶刊选择</h2><ul><li><p>基本上中科院1区的都是顶刊</p></li><li><p>Xmol中有分类 如图<img src="F:\Typora图\image-20240723073704311.png" alt="image-20240723073704311"></p></li></ul><h2 id="中文科技论文格式"><a href="#中文科技论文格式" class="headerlink" title="中文科技论文格式"></a>中文科技论文格式</h2><p>1、字体和字号<br>论文题目：二号黑体，居中<br>各章标题：小二号黑体，居左<br>各节一级标题：小三号黑体，居左<br>各节二级标题：四号黑体，居左<br>各节三级标题：小四号黑体，居左<br>正文：小四号宋体</p><p>2、行距：正文为1.5倍行距，段前、段后无空行，各段落首行缩进2字符</p><h1 id="复制粘贴闹麻了"><a href="#复制粘贴闹麻了" class="headerlink" title="复制粘贴闹麻了"></a>复制粘贴闹麻了</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;需求如下&quot;&gt;&lt;a href=&quot;#需求如下&quot; class=&quot;headerlink&quot; title=&quot;需求如下&quot;&gt;&lt;/a&gt;需求如下&lt;/h1&gt;&lt;img src=&quot;C:&#92;Users&#92;huangruiwen&#92;AppData&#92;Roaming&#92;Typora&#92;typora-user</summary>
      
    
    
    
    
    <category term="论文" scheme="https://1kunn.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>小土堆pytorch</title>
    <link href="https://1kunn.github.io/2024/07/22/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch/"/>
    <id>https://1kunn.github.io/2024/07/22/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch/</id>
    <published>2024-07-22T12:18:28.000Z</published>
    <updated>2024-08-04T09:04:13.287Z</updated>
    
    <content type="html"><![CDATA[<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><h2 id="视频链接："><a href="#视频链接：" class="headerlink" title="视频链接："></a>视频链接：</h2><p><a href="https://www.bilibili.com/video/BV1hE411t7RN/?p=8&spm_id_from=333.880.my_history.page.click&vd_source=49378ece86ab83ee76bcaac1cc96562c">https://www.bilibili.com/video/BV1hE411t7RN/?p=8&amp;spm_id_from=333.880.my_history.page.click&amp;vd_source=49378ece86ab83ee76bcaac1cc96562c</a></p><p>其他代码 数据集等链接在视频详情页</p><p>2024.7.20实测仍能使用</p><h1 id="TensorBoard的使用（一）"><a href="#TensorBoard的使用（一）" class="headerlink" title="TensorBoard的使用（一）"></a>TensorBoard的使用（一）</h1><ul><li><p>python3.6环境</p></li><li><p>代码（仅仅是构建的框架 构建y&#x3D;x的图）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="comment">#引入summarywriter工具 按住ctrl可以查看包的说明</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>) <span class="comment">#指的是对应内容存储到logs文件夹下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># writer.add_image()</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;y=x&#x27;</span>, i, i) <span class="comment">#参数分别是 标题 y轴 x轴</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=learn/logs --port=<span class="number">6007</span> <span class="comment">#控制台打开在6007端口处 环境要安装tensorboard  learn/logs是文件的相对路径 图像如下图所示</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="C:\Users\huangruiwen\AppData\Roaming\Typora\typora-user-images\image-20240722214624078.png" alt="image-20240722214624078"></p><h1 id="TensorBoard的使用（二）"><a href="#TensorBoard的使用（二）" class="headerlink" title="TensorBoard的使用（二）"></a>TensorBoard的使用（二）</h1><p>writer.add_image需要str numpy.array等数据型 可以在说明中看到</p><p>路径要绝对路径才生效 我也不懂为啥</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="comment">#引入summarywriter工具 按住ctrl可以查看包的说明</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>) <span class="comment">#指的是对应内容存储到logs文件夹下</span></span><br><span class="line"></span><br><span class="line">image_path = <span class="string">&quot;D:\\program\\Anaconda3\\envs\\tf-gpu1\\learn\\data\\train\\ants_image\\0013035.jpg&quot;</span> <span class="comment">#图片路径</span></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(image_path) <span class="comment">#打开图片</span></span><br><span class="line">img_array = np.array(img_PIL) <span class="comment">#数据类型转换 因为writer.add_image()数据类型要求</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&quot;test&quot;</span>, img_array, dataformats=<span class="string">&#x27;HWC&#x27;</span>) <span class="comment">#参数分别是 标题 要打开的数据 第三个我也不懂</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;y=x&#x27;</span>, i, i) <span class="comment">#参数分别是 标题 y轴 x轴</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="效果如下"><a href="#效果如下" class="headerlink" title="效果如下"></a>效果如下</h2><p><img src="F:\Typora图\image-20240725085924903.png" alt="image-20240725085924903"></p><h1 id="transforms的使用"><a href="#transforms的使用" class="headerlink" title="transforms的使用"></a>transforms的使用</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># python的用法 -&gt; tensor数据类型</span></span><br><span class="line"><span class="comment">#通过transforms。ToTensor了解 transforms如何使用</span></span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;D:\\program\\Anaconda3\\envs\\tf-gpu1\\dataset\\train\\ants\\5650366_e22b7e1065.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path) <span class="comment">#打开图片</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#1 如何使用</span></span><br><span class="line">tensor_trans = transforms.ToTensor() <span class="comment">#先自己创建一个工具tensor_trans 返回Totensor类型的数据 因为工具包需要</span></span><br><span class="line">tensor_img = tensor_trans(img) <span class="comment">#通过创造出的工具转化数据类型</span></span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&quot;Tensor_img&quot;</span>, tensor_img)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="常见的transforms"><a href="#常见的transforms" class="headerlink" title="常见的transforms"></a>常见的transforms</h1><p>简单记录下 使用还得后面多实践</p><p>使用方法都类似 要关注数据类型转变</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;D:\\program\\Anaconda3\\envs\\tf-gpu1\\images\\8561027.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#ToTensor用法</span></span><br><span class="line">trans_totensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_totensor(img)</span><br><span class="line">writer.add_image(<span class="string">&quot;ToTensor&quot;</span>, img_tensor)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Normalize归一化 改颜色的</span></span><br><span class="line">trans_norm = transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>],[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">img_norm = trans_norm(img_tensor)</span><br><span class="line">writer.add_image(<span class="string">&quot;Normalize&quot;</span>, img_norm)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Resize 改规格</span></span><br><span class="line">trans_resize = transforms.Resize((<span class="number">512</span>, <span class="number">512</span>)) <span class="comment">#img PIL -&gt; resize -&gt;img_resize PIL</span></span><br><span class="line">img_resize = trans_resize(img) <span class="comment">#img_resize PIL -&gt; totensor -&gt; img_resize</span></span><br><span class="line">img_resize = trans_totensor(img_resize)</span><br><span class="line">writer.add_image(<span class="string">&quot;Resize&quot;</span>, img_resize, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Compose -resize 也是改规格 要关注输入输出数据类型</span></span><br><span class="line">trans_resize_2 = transforms.Resize(<span class="number">512</span>)</span><br><span class="line"><span class="comment"># PIL -&gt; PIL -&gt; tensor</span></span><br><span class="line">trans_compose = transforms.Compose([trans_resize_2, trans_totensor]) <span class="comment">#compose()中参数要一个列表 所以用trans_totensor转换数据类型</span></span><br><span class="line">img_resize_2 =trans_compose(img)</span><br><span class="line">writer.add_image(<span class="string">&quot;Resize&quot;</span>, img_resize_2, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#RandomCrop 随机裁剪</span></span><br><span class="line">trans_random = transforms.RandomCrop(<span class="number">512</span>)</span><br><span class="line">trans_compose_2 = transforms.Compose([trans_random, trans_totensor])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment">#随机裁剪10个</span></span><br><span class="line">    img_crop = trans_compose_2(img)</span><br><span class="line">    writer.add_image(<span class="string">&quot;RandomCrop&quot;</span>, img_crop, i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="torchvision中的数据集使用"><a href="#torchvision中的数据集使用" class="headerlink" title="torchvision中的数据集使用"></a>torchvision中的数据集使用</h1><p><img src="F:\Typora图\image-20240728085901802-17221283448361.png" alt="image-20240728085901802"></p><p>pytorch官网这里能找到官方数据集下载</p><p>数据集下载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>) <span class="comment">#调用数据集 参数：存放路径 是否是训练数据集 是否下载</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>) <span class="comment">#测试数据集 只需要调整train为False</span></span><br></pre></td></tr></table></figure><p>数据集下载会到这</p><p><img src="F:\Typora图\image-20240728104958162-17221349999463.png" alt="image-20240728104958162"></p><p>数据集使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment">#transform的调用 有点像c里的函数</span></span><br><span class="line">dataset_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()<span class="comment">#转成totensor</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=dataset_transform, download=<span class="literal">True</span>) <span class="comment">#调用数据集 参数：存放路径 是否是训练数据集 是否下载</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=dataset_transform, download=<span class="literal">True</span>) <span class="comment">#测试数据集 只需要调整train为False</span></span><br><span class="line"><span class="comment">#运行后所有数据都转为totensor型</span></span><br><span class="line"><span class="comment">#第三个参数就是调用transform</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;p10&#x27;</span>)<span class="comment">#保存</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment">#显示前10张</span></span><br><span class="line">    img, target = test_set[i]</span><br><span class="line">    writer.add_image(<span class="string">&#x27;test_set&#x27;</span>, img, i) <span class="comment">#标签 tensor数据的图片 下标</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="DataLoader的使用"><a href="#DataLoader的使用" class="headerlink" title="DataLoader的使用"></a>DataLoader的使用</h1><p>相比datase dataloader会把数据加载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备的测试数据集</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor())<span class="comment">#引入测试数据集</span></span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">True</span>)<span class="comment">#参数： 数据集 每次取4个打包</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据集中第一张图片及target</span></span><br><span class="line">img, target = test_data[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img.shape)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;dataloader&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        <span class="comment"># print(imgs.shape)</span></span><br><span class="line">        <span class="comment"># print(targets)</span></span><br><span class="line">        writer.add_images(<span class="string">&quot;Epoch: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch), imgs, step)  <span class="comment">#参数： 命名 img 步进</span></span><br><span class="line">        step = step + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>效果：</p><p><img src="F:\Typora图\image-20240729102408512-17222198505101.png" alt="image-20240729102408512"></p><h1 id="神经网络各层次"><a href="#神经网络各层次" class="headerlink" title="神经网络各层次"></a>神经网络各层次</h1><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>有些超纲 看不懂 各项参数设置还得看官方文档 真要用的时候再了解</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, <span class="variable language_">self</span>).__init__() <span class="comment">#父类初始化</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>) <span class="comment">#卷积层设置</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment">#输出</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">tudui = Tudui()</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;../logs&quot;</span>)</span><br><span class="line"></span><br><span class="line">step = <span class="number">0</span> <span class="comment">#步进</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = tudui(imgs)</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line">    <span class="comment"># torch.Size([64, 3, 32, 32])</span></span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, step) <span class="comment">#看变化</span></span><br><span class="line">    <span class="comment"># torch.Size([64, 6, 30, 30])  -&gt; [xxx, 3, 30, 30]</span></span><br><span class="line"></span><br><span class="line">    output = torch.reshape(output, (-<span class="number">1</span>, <span class="number">3</span>, <span class="number">30</span>, <span class="number">30</span>)) <span class="comment">#尺寸变化 因为原尺寸无法显示</span></span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>, output, step) <span class="comment">#看变化</span></span><br><span class="line"></span><br><span class="line">    step = step + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="最大池化层（下采样）"><a href="#最大池化层（下采样）" class="headerlink" title="最大池化层（下采样）"></a>最大池化层（下采样）</h2><p>为了在保留特征的前提下缩小输入数据大小</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module): <span class="comment">#初始化神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.maxpool1 = MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = <span class="variable language_">self</span>.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">tudui = Tudui()</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;../logs_maxpool&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, step)</span><br><span class="line">    output = tudui(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>, output, step)</span><br><span class="line">    step = step + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="非线性激活"><a href="#非线性激活" class="headerlink" title="非线性激活"></a>非线性激活</h2><p>ReLu Sigmoid 那些</p><h1 id="神经网络搭建实战（CIFAR10）"><a href="#神经网络搭建实战（CIFAR10）" class="headerlink" title="神经网络搭建实战（CIFAR10）"></a>神经网络搭建实战（CIFAR10）</h1><p>CIFAR10结构</p><p><img src="F:\Typora图\image-20240729104118826-17222208838803.png" alt="image-20240729104118826"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )   <span class="comment">#导入各层结构 具体数值设置看视频推演 很有用</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x <span class="comment">#输出结果</span></span><br><span class="line"></span><br><span class="line">tudui = Tudui()</span><br><span class="line"><span class="built_in">print</span>(tudui)</span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">output = tudui(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;../logs_seq&quot;</span>)</span><br><span class="line">writer.add_graph(tudui, <span class="built_in">input</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="现有网络模型的使用及修改"><a href="#现有网络模型的使用及修改" class="headerlink" title="现有网络模型的使用及修改"></a>现有网络模型的使用及修改</h1><p>pytorch官网</p><p><img src="F:\Typora图\image-20240730152627846-17223243907721.png" alt="image-20240730152627846"></p><h1 id="网络模型的保存和加载"><a href="#网络模型的保存和加载" class="headerlink" title="网络模型的保存和加载"></a>网络模型的保存和加载</h1><p>不同的保存方式要用不同的加载方式</p><p>保存代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 保存方式1,模型结构+模型参数（保存了两个东西）</span></span><br><span class="line">torch.save(vgg16, <span class="string">&quot;vgg16_method1.pth&quot;</span>) <span class="comment">#命名  保存路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存方式2，模型参数（官方推荐 只保存模型参数）</span></span><br><span class="line">torch.save(vgg16.state_dict(), <span class="string">&quot;vgg16_method2.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 陷阱</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">tudui = Tudui()</span><br><span class="line">torch.save(tudui, <span class="string">&quot;tudui_method1.pth&quot;</span>)</span><br></pre></td></tr></table></figure><p>加载代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> model_save <span class="keyword">import</span> *</span><br><span class="line"><span class="comment"># 方式1-》保存方式1，加载模型</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">&quot;vgg16_method1.pth&quot;</span>)</span><br><span class="line"><span class="comment"># print(model)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2，加载模型 对应的保存方式对应的加载方式</span></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg16.load_state_dict(torch.load(<span class="string">&quot;vgg16_method2.pth&quot;</span>))</span><br><span class="line"><span class="comment"># model = torch.load(&quot;vgg16_method2.pth&quot;)</span></span><br><span class="line"><span class="comment"># print(vgg16)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 陷阱1</span></span><br><span class="line"><span class="comment"># class Tudui(nn.Module):</span></span><br><span class="line"><span class="comment">#     def __init__(self):</span></span><br><span class="line"><span class="comment">#         super(Tudui, self).__init__()</span></span><br><span class="line"><span class="comment">#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     def forward(self, x):</span></span><br><span class="line"><span class="comment">#         x = self.conv1(x)</span></span><br><span class="line"><span class="comment">#         return x</span></span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">&#x27;tudui_method1.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><h1 id="完整的模型训练套路"><a href="#完整的模型训练套路" class="headerlink" title="完整的模型训练套路"></a>完整的模型训练套路</h1><p>主代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> * <span class="comment">#引入model.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#完整的模型训练</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用DataLoader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">tudui = Tudui()</span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 如果train_data_size=10, 训练数据集的长度为：10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line"><span class="comment"># learning_rate = 0.01</span></span><br><span class="line"><span class="comment"># 1e-2=1 x (10)^(-2) = 1 /100 = 0.01</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(tudui.parameters(), lr=learning_rate) <span class="comment">#对哪部分进行优化 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置训练网络的一些参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#训练的轮数</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;../logs_train&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line"></span><br><span class="line">    <span class="comment">#训练步骤开始</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data <span class="comment">#取得训练数据</span></span><br><span class="line">        output = tudui(imgs) <span class="comment">#得到输出</span></span><br><span class="line">        loss = loss_fn(output, targets) <span class="comment">#看输出与实际的误差</span></span><br><span class="line">        <span class="comment">#优化器优化模型</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#梯度清零</span></span><br><span class="line">        loss.backward() <span class="comment">#反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#优化</span></span><br><span class="line"></span><br><span class="line">        total_train_step = total_train_step + <span class="number">1</span>  <span class="comment"># 训练次数加一</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>: <span class="comment">#100次训练打印一次</span></span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;, Loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>, loss.item(), total_train_step)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试步骤开始 为了看训练后的模型的准确性</span></span><br><span class="line">    tudui.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader: <span class="comment">#取测试数据集</span></span><br><span class="line">            imgs, targets = data</span><br><span class="line">            outputs = tudui(imgs) <span class="comment">#对应的输出</span></span><br><span class="line">            loss = loss_fn(outputs, targets) <span class="comment">#比对误差</span></span><br><span class="line">            total_test_loss = total_test_loss + loss.item()</span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的Loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_accuracy/test_data_size, total_test_step)</span><br><span class="line">    total_test_step = total_test_step + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    torch.save(tudui, <span class="string">&quot;tudui_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>model设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#搭建神经网络 CIFAR10有十个类 所以神经网络要设为十分类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.model(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    tuidu = Tudui()</span><br><span class="line">    <span class="built_in">input</span> = torch.ones((<span class="number">64</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">    output= tuidu(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>((output.shape))</span><br></pre></td></tr></table></figure><h1 id="利用GPU加速"><a href="#利用GPU加速" class="headerlink" title="利用GPU加速"></a>利用GPU加速</h1><p>其实就是比cpu（上面的版本） 在模型 数据 损失函数中利用cuda进行gpu加速</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 作者：小土堆</span></span><br><span class="line"><span class="comment"># 公众号：土堆碎念</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># from model import *</span></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 如果train_data_size=10, 训练数据集的长度为：10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用 DataLoader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.model(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">tudui = Tudui()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    tudui = tudui.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    loss_fn = loss_fn.cuda()</span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line"><span class="comment"># learning_rate = 0.01</span></span><br><span class="line"><span class="comment"># 1e-2=1 x (10)^(-2) = 1 /100 = 0.01</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(tudui.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练网络的一些参数</span></span><br><span class="line"><span class="comment"># 记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 训练的轮数</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;../logs_train&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-------第 &#123;&#125; 轮训练开始-------&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    tudui.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            imgs = imgs.cuda()</span><br><span class="line">            targets = targets.cuda()</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 优化器优化模型</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_train_step = total_train_step + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;, Loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>, loss.item(), total_train_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤开始</span></span><br><span class="line">    tudui.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                targets = targets.cuda()</span><br><span class="line">            outputs = tudui(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line">            total_test_loss = total_test_loss + loss.item()</span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的Loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_accuracy/test_data_size, total_test_step)</span><br><span class="line">    total_test_step = total_test_step + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    torch.save(tudui, <span class="string">&quot;tudui_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="完整的模型验证（测试）套路"><a href="#完整的模型验证（测试）套路" class="headerlink" title="完整的模型验证（测试）套路"></a>完整的模型验证（测试）套路</h1><p>利用已经训练好的模型，给它提供输入</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;链接&quot;&gt;&lt;a href=&quot;#链接&quot; class=&quot;headerlink&quot; title=&quot;链接&quot;&gt;&lt;/a&gt;链接&lt;/h1&gt;&lt;h2 id=&quot;视频链接：&quot;&gt;&lt;a href=&quot;#视频链接：&quot; class=&quot;headerlink&quot; title=&quot;视频链接：&quot;&gt;&lt;/a&gt;视频链接</summary>
      
    
    
    
    
    <category term="学习记录" scheme="https://1kunn.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>学习Typora</title>
    <link href="https://1kunn.github.io/2024/07/21/%E5%AD%A6%E4%B9%A0Typora/"/>
    <id>https://1kunn.github.io/2024/07/21/%E5%AD%A6%E4%B9%A0Typora/</id>
    <published>2024-07-21T01:00:08.000Z</published>
    <updated>2024-07-22T12:17:34.049Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Typora语法"><a href="#Typora语法" class="headerlink" title="Typora语法"></a>Typora语法</h1><p>#就是一级标题</p><p>##二级标题，以此类推</p><h1 id="行间公式"><a href="#行间公式" class="headerlink" title="行间公式"></a>行间公式</h1><p> 输入$$然后按回车<br>$$<br>F&#x3D;ma<br>$$</p><h1 id="列表生成"><a href="#列表生成" class="headerlink" title="列表生成"></a>列表生成</h1><p>-（横杠）和一个空格 </p><h2 id=""><a href="#" class="headerlink" title="- "></a>- </h2><p>数字和点和空格就是有序列表</p><p>1. </p><h1 id="插入代码块"><a href="#插入代码块" class="headerlink" title="插入代码块"></a>插入代码块</h1><p>3个反引号+回车</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> rang(<span class="number">5</span>):</span><br></pre></td></tr></table></figure><h1 id="插入图片-表格-都在上面格式里"><a href="#插入图片-表格-都在上面格式里" class="headerlink" title="插入图片 表格 都在上面格式里"></a>插入图片 表格 都在上面格式里</h1><h1 id="文本格式"><a href="#文本格式" class="headerlink" title="文本格式"></a>文本格式</h1><p>左右都加</p><p>加粗是两个*</p><p><strong>加粗</strong></p><p>斜体是一个*</p><p><em>斜体</em></p><p>下划线 ctrl+u</p><p><u>下划线</u></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Typora语法&quot;&gt;&lt;a href=&quot;#Typora语法&quot; class=&quot;headerlink&quot; title=&quot;Typora语法&quot;&gt;&lt;/a&gt;Typora语法&lt;/h1&gt;&lt;p&gt;#就是一级标题&lt;/p&gt;
&lt;p&gt;##二级标题，以此类推&lt;/p&gt;
&lt;h1 id=&quot;行间公式&quot;&gt;&lt;</summary>
      
    
    
    
    
    <category term="学习记录" scheme="https://1kunn.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>MyfirstBlog</title>
    <link href="https://1kunn.github.io/2024/07/20/MyfirstBlog/"/>
    <id>https://1kunn.github.io/2024/07/20/MyfirstBlog/</id>
    <published>2024-07-20T12:10:48.000Z</published>
    <updated>2024-07-21T00:50:32.487Z</updated>
    
    <content type="html"><![CDATA[<h1 id="这是我的第一篇博客"><a href="#这是我的第一篇博客" class="headerlink" title="这是我的第一篇博客"></a>这是我的第一篇博客</h1><p>欢迎你！</p><p> 是我 如果我有多张船film 你会唔会同我一齐走</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;这是我的第一篇博客&quot;&gt;&lt;a href=&quot;#这是我的第一篇博客&quot; class=&quot;headerlink&quot; title=&quot;这是我的第一篇博客&quot;&gt;&lt;/a&gt;这是我的第一篇博客&lt;/h1&gt;&lt;p&gt;欢迎你！&lt;/p&gt;
&lt;p&gt; 是我 如果我有多张船film 你会唔会同我一齐走&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="tag1" scheme="https://1kunn.github.io/tags/tag1/"/>
    
  </entry>
  
</feed>
